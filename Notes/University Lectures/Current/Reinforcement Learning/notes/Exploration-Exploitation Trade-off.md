- Exploration: Improve knowledge for long-term benefit
- Exploitation: Exploit knowledge for short-term benefit

## Example
- Assume this estimations of a [[K-Armed Bandit Problem]] after 5 sampling steps:
![[Pasted image 20240310095707.png#invert|500]]
- The following shows which action selections would be exploration or exploitation:
![[Pasted image 20240310095829.png#invert|500]]


## Solutions
A purely greedy policy can get stuck in sub-optimal solutions because it always exploits the current best-known action without exploring other potentially better options. This lack of exploration can prevent the policy from finding the optimal action if it hasnâ€™t been tried enough to reveal its true value.
1. [[Epsilon-Greedy Action Selection]]
2. [[Exploration and Entropy]]
3. [[Optimistic Value Initialization]]
4. [[Upper-Confidence-Bounds (UCB) Action Selection]]