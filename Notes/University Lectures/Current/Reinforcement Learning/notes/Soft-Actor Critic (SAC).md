The Soft Actor-Critic (SAC) algorithm is a reinforcement learning method that <mark style="background: #FFB86CA6;">combines the benefits of actor-critic methods with improvements in exploration, by entropy maximization</mark>. This technique <mark style="background: #FFB86CA6;">optimizes a policy that seeks to maximize not only the expected sum of rewards but also the entropy of the policy</mark>. This addition of entropy <mark style="background: #FFB86CA6;">ensures more exploration by the policy, preventing premature convergence</mark> to suboptimal deterministic policies.
### Critic: “Soft” Q-Function
In SAC, <mark style="background: #FFB86CA6;">the Q-function is augmented with an entropy term to form "soft" Q-function</mark>:

$$ 
y_t = r_t + \gamma \left( \min_{j=1,2} Q_{\phi_j}(s', a') - \alpha \log \pi(a'|s') \right)
$$
#### Key Elements:
- **Entropy Bonus:** The term $\alpha \log \pi(a' \mid s')$ where $\alpha$ is the temperature parameter that scales the importance of the entropy term against the reward. <mark style="background: #FFB86CA6;">This promotes exploration by rewarding uncertainty in action selection</mark>.
- **Double Q-Learning:** SAC typically uses <mark style="background: #FFB86CA6;">two Q-functions</mark> (denoted as $Q_{\phi_1}$ and $Q_{\phi_2}$) to mitigate positive bias in the policy improvement step, similar to the Clipped Double Q-Learning in TD3. It <mark style="background: #FFB86CA6;">takes the minimum of these two Q-functions to reduce overestimation</mark>.
### Actor: Policy Optimization
The <mark style="background: #FFB86CA6;">policy is updated to maximize a trade-off between expected return and entropy</mark>, <mark style="background: #FFB86CA6;">encouraging the policy to explore</mark> more widely:

$$ 
J_{SAC}(\theta) = \mathbb{E}_{s \sim \mathcal{D}} \left[ \mathbb{E}_{a \sim \pi_\theta} \left[ Q_\phi(s, a) - \alpha \log \pi_\theta(a \mid s) \right] \right]
$$
#### Optimization Techniques:
- **Reparameterization Trick:** SAC uses this trick to <mark style="background: #FFB86CA6;">improve sample efficiency and reduce variance in policy gradients</mark>. Actions are sampled according to $a = f_\theta(\epsilon; s)$ where $\epsilon$ is an <mark style="background: #FFB86CA6;">input noise</mark> vector, and $f_\theta$ is a <mark style="background: #FFB86CA6;">deterministic function</mark>. This <mark style="background: #FFB86CA6;">allows gradients to pass through the stochastic node of action sampling</mark>.
### Reparameterization Trick
The reparameterization trick enables the gradients of the stochastic policy to be computed more effectively:

$$ 
a = \mu_\theta(s) + \sigma_\theta(s) \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
$$

where $\mu$ and $\sigma$ are the outputs of the policy network, and $\epsilon$ is an input noise vector. This formulation helps in computing gradients directly with respect to policy parameters $\theta$ by making the randomness independent of the parameters.
### Actor Update in SAC
#### Actor Objective
The objective function for the actor in SAC aims to maximize the expected reward while also maximizing entropy to encourage exploration. The mathematical formulation provided in your image is:

$$ J_{SAC}(\theta) \approx \frac{1}{N} \sum_{i} \mathbb{E}_{a \sim \pi_\theta} \left[Q_\phi(s_i, a) - \alpha \log \pi_\theta(a \mid s_i)\right] $$

Here:
- $\pi_\theta(a \mid s)$ is the policy parameterized by $\theta$.
- $Q_\phi(s, a)$ is the soft Q-function that outputs the expected return of taking action $a$ in state $s$, adjusted by the entropy term.
- $\alpha$ is the temperature parameter that controls the trade-off between entropy and reward.
#### Reparameterization Trick
- **Purpose**: This trick is used to allow backpropagation through stochastic nodes. It helps to compute gradients of the policy parameters $\theta$ efficiently.
- **Implementation**: Assume the policy $\pi_\theta$ is Gaussian, actions are sampled as $a = \mu_\theta(s) + \sigma_\theta(s) \cdot \epsilon$, where $\epsilon \sim \mathcal{N}(0, I)$. Here, $\mu_\theta(s)$ and $\sigma_\theta(s)$ are outputs of the neural network representing the mean and standard deviation of the policy's action distribution.
- **Gradient Calculation**: The reparameterization allows the gradient of the objective with respect to the parameters $\theta$ to be computed as if $a$ were deterministically generated by $\theta$ and $\epsilon$.
### Advantages of SAC
1. **Exploration-Exploitation Balance:** By <mark style="background: #FFB86CA6;">maximizing entropy, SAC naturally balances exploration and exploitation</mark>, leading to more robust learning in environments with sparse or deceptive rewards.
2. **Stability and Convergence:** The <mark style="background: #FFB86CA6;">use of double Q-functions and the reparameterization trick contributes to the stability</mark> of the learning process and often leads to <mark style="background: #FFB86CA6;">faster convergence</mark> compared to methods that do not incorporate these features.
3. **Continuous Action Spaces:** SAC is <mark style="background: #FFB86CA6;">particularly well-suited for tasks with continuous action spaces</mark>, making it ideal for complex control tasks such as robotics.
## SAC Algorithm
![[Pasted image 20240708123401.png#invert|600]]
1. **Data Collection**: Interact with the environment using the current policy and store the tuples $(s_i, a_i, r_i, s'_i)$ in a replay buffer.
2. **Sample a Batch**: Randomly sample a batch of transitions from the buffer to use for training.
3. **Compute Target Values**: For each sampled transition, compute the target value for the Q-function updates using:
   $$ y_i = r_i + \gamma \left( \min_{j=1,2} Q_{\phi_j}(s'_i, a') - \alpha \log \pi(a' \mid s'_i) \right) $$
   where $a'$ is sampled from the current policy.
4. **Update Q-Function**: Adjust the parameters of the Q-functions by minimizing the mean squared error between the current Q-function estimates and the computed target values.
5. **Update Policy**: Use the reparameterization trick and the policy gradient derived from the actor's objective to update the policy parameters $\theta$.
6. **Update Target Networks**: Use Polyak averaging to slowly update the target Q-function parameters towards the current Q-function parameters. This stabilizes learning by making the target function change more smoothly.
