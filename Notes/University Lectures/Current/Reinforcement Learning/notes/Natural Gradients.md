> [!tldr] Definition
> Natural gradients are a method used in reinforcement learning to adjust the policy parameters in a way that takes into account the underlying probability distribution of the policy. This approach is <mark style="background: #FFB86CA6;">more efficient than standard gradients</mark> because it scales the gradient by the inverse of the [[Fisher Information Matrix (FIM)]], which leads to faster convergence. 

- <mark style="background: #FFB86CA6;">Natural Gradients use a Taylor approximation of the trust region problem, with a first-order Taylor for the objective and a second-order Taylor for the constraint</mark>
- The trust region method in policy optimization aims to update the policy parameters so that the new policy is not too far from the old policy, ensuring stable improvements. This is formalized as:$$
   \max_{g} g^T \nabla_\theta J \quad \text{subject to} \quad KL(p_{\theta_{old} + g} \| p_{\theta_{old}}) \approx g^T \mathcal{F} g \leq \epsilon,
   $$where:
   - $g$ is the gradient of the objective function $J$ concerning the policy parameters $\theta$,
   - $KL(p_{\theta_{old} + g} \| p_{\theta_{old}})$ is the [[Kullback-Leiber Divergence]] between the new policy and the old policy, approximated as a quadratic form of the gradient.
### Taylor Approximations
- <mark style="background: #FFB86CA6;">First-order Taylor approximation for the objective function</mark> gives the direction of steepest ascent.: $$g_{NG}=\arg\max_{g}g^{T}\nabla_{\theta}J$$
- <mark style="background: #FFB86CA6;">Second-order Taylor approximation for the constraint (the KL divergence) using the FIM</mark> provides a quadratic form, establishing a trust region that limits how far the updated policy can deviate from the old policy.: $$KL(p_{\theta_{old}+g}\parallel p_{\theta_{old}})\approx g^{T}\mathcal{F}g\le\epsilon$$, with $\mathcal{F}$ being the Fisher Information Matrix (FIM): $$\mathcal{F}=\frac{\partial KL(p_\theta\parallel p_{\theta_{old}})}{\partial\theta\partial\theta}$$
### Applying the FIM in Natural Gradient
The natural gradient, $g_{NG}$, modifies the 'vanilla' gradient $\nabla_\theta J$ by scaling it with the inverse of the FIM:
$$ g_{NG} = \eta F^{-1} \nabla_\theta J, $$
where $\eta$ is a step size or learning rate.

1. **Vanilla Gradient**: The gradient of the policy objective function, denoted as $\nabla_\theta J(\theta)$, is given by:
   $$
   \nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i} \sum_{t} \frac{\pi_\theta(a_{i,t}|s_{i,t})}{\pi_{\text{old}}(a_{i,t}|s_{i,t})} \nabla_\theta \log \pi_\theta(a_{i,t}|s_{i,t}) \hat{A}^{\pi_{\text{old}}}(s_{i,t}, a_{i,t})
   $$
   - **Explanation**: This expression estimates the policy gradient by averaging over a sample of trajectories, where each term in the sum involves adjusting the log probability of the selected action by its advantage ($\hat{A}$), scaled by the likelihood ratio between the new and old policies.
2. **KL Constraint**: <mark style="background: #FFB86CA6;">Ideally, you would want to constrain each state's policy update by limiting the KL divergence between the new and old policies for every state</mark>. However, this is <mark style="background: #FFB86CA6;">often impractical</mark> because it requires constraints for potentially every state in the state space.
   $$
   KL(\pi_\theta(a|s) \| \pi_{\text{old}}(a|s)) \leq \epsilon, \quad \forall s \in S
   $$
   - **Explanation**: This constraint would ensure minimal deviation from the old policy on a state-by-state basis, promoting stability but being too restrictive and computationally complex.
### Relaxed Expected Constraint
Rather than applying the KL constraint at every state, a relaxed form is used where the <mark style="background: #FFB86CA6;">expected KL divergence across the state distribution generated by the old policy is constrained</mark>. This approach balances between exploration and exploitation by allowing some states to have higher deviations as long as the overall expectation remains within a defined threshold.
   $$
   \mathbb{E}_{\mu^{\pi_{\text{old}}}(s)} [KL(\pi_\theta(a|s) \| \pi_{\text{old}}(a|s))] \leq \epsilon
   $$
   - **Explanation**: This constraint <mark style="background: #FFB86CA6;">averages the KL divergence across all states</mark> visited under the old policy, making it <mark style="background: #FFB86CA6;">more computationally feasible and less stringent while still maintaining some control over the policy update's impact</mark>.
### Approximation Using the Fisher Information Matrix
The Fisher Information Matrix (FIM), denoted $F$, provides a way to <mark style="background: #FFB86CA6;">approximate this relaxed constraint</mark>:
   $$
   F = \mathbb{E}_{\mu^{\pi_{\text{old}}}(s)} \left[ \nabla_\theta \log \pi_\theta(a|s) \nabla_\theta \log \pi_\theta(a|s)^T \right]
   $$
   - **Explanation**: $F$ captures the expected outer product of the gradients of the log probabilities of actions, averaging over the state distribution. <mark style="background: #FFB86CA6;">This matrix quantifies how much information each parameter of the policy contributes to the changes in action probabilities, essentially measuring the sensitivity of the policy to parameter changes</mark>.
### Trust Region Problem and Natural Gradient
The natural gradient, $g_{NG}$, is then computed by solving a trust region problem that uses the Fisher Information Matrix to scale the vanilla gradient:
   $$
   g_{NG} = \arg\max_g g^T \nabla_\theta J \quad \text{s.t.} \quad g^T F g \leq \epsilon
   $$
   - **First-Order Taylor for the Objective**: Directly uses the gradient of the objective.
   - **Second-Order Taylor for the Constraint**: Approximates the KL divergence between the policy parameterized by $\theta_{old} + g$ and $\theta_{old}$ using the FIM.
### Optimization Process
1. **Compute the Gradient**: Calculate the standard policy gradient $\nabla_\theta J$.
2. **Adjust by FIM**: Scale this gradient by the inverse of the FIM to account for the trust region constraint.
3. **Update the Parameters**: Update the policy parameters using this adjusted gradient to ensure that changes in the policy do not move too far from the previous policy, adhering to the KL divergence constraint.
