{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Setup\n",
    "\n",
    "If you prefer to work locally, see the following instructions for setting up Python in a virtual environment.\n",
    "You can then ignore the instructions in \"Colab Setup\".\n",
    "\n",
    "If you haven't yet, create a [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html) environment using:\n",
    "```\n",
    "conda create --name rl_exercises\n",
    "conda activate rl_exercises\n",
    "```\n",
    "Torch recommends installation using conda rather than pip, so run e.g.:\n",
    "```\n",
    "conda install pytorch pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "```\n",
    "For this exercise, you require a CUDA-enabled GPU, as training an image-based model on the CPU takes a very long time.\n",
    "Visit [the installation page](https://pytorch.org/get-started/locally/) to see the options available for different CUDA versions.\n",
    "The remaining dependencies can be installed with pip:\n",
    "```\n",
    "pip install matplotlib numpy tqdm ipykernel \"gymnasium[atari, accept-rom-license, classic-control, other]\" stable-baselines3\n",
    "```\n",
    "\n",
    "Even if you are running the Jupyter notebook locally, please run the code cells in **Colab Setup**, because they define some global variables required later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab Setup\n",
    "\n",
    "Google Colab provides you with a temporary environment for python programming.\n",
    "While this conveniently works on any platform and internally handles dependency issues and such, it also requires you to set up the environment from scratch every time.\n",
    "The \"Colab Setup\" section below will be part of **every** exercise and contains utility that is needed before getting started.\n",
    "\n",
    "**IMPORTANT**: For this exercise, you require a GPU runtime environment, as training an image-based model on the CPU takes a very long time.\n",
    "To do this, select \"Change runtime type\" from the context menu in the top right corner (next to the **Connect** button), and select **T4 GPU**.\n",
    "\n",
    "There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window).\n",
    "Any changes you make to the Jupyter notebook itself should be saved to your Google Drive.\n",
    "We also save all recordings and logs in it by default so that you won't lose your work in the event of an instance timeout.\n",
    "However, you will need to re-mount your Google Drive and re-install packages with every new instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Your work will be stored in a folder called `rl_ws23` by default to prevent Colab \n",
    "instance timeouts from deleting your edits.\n",
    "We do this by mounting your google drive on the virtual machine created in this colab \n",
    "session. For this, you will likely need to sign in to your Google account and allow\n",
    "access to your Google Drive files.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    COLAB = True\n",
    "except ImportError:\n",
    "    COLAB = False\n",
    "\n",
    "# Create paths in your google drive\n",
    "if COLAB:\n",
    "    DATA_ROOT = Path(\"/content/gdrive/My Drive/rl_ws23\")\n",
    "    DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    DATA_ROOT_STR = str(DATA_ROOT)\n",
    "    %cd \"$DATA_ROOT\"\n",
    "else:\n",
    "    DATA_ROOT = Path.cwd() / \"rl_ws23\"\n",
    "\n",
    "# Install python packages\n",
    "if COLAB:\n",
    "    %pip install matplotlib numpy tqdm \"gymnasium[atari, accept-rom-license, classic-control, other]\" stable-baselines3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UunygyDXrx7k"
   },
   "source": [
    "# Exercise 2\n",
    "\n",
    "Designed by Ge Li (ge.li@kit.edu) and Bal√°zs Gyenes, inspired by the official PyTorch DQN implementation and the cleanrl implementation of DQN on Atari.\n",
    "\n",
    "In this homework, we are going to implement the Deep Q-Network algorithm and apply it to the Atari 2600 game \"Breakout\".\n",
    "Atari was a popular game console during the 1980s, and Breakout is a game where few layers of bricks line the top of the screen and the goal is to destroy them all by repeatedly bouncing a ball off a paddle into them.\n",
    "\n",
    "![The Breakout Environment](https://gymnasium.farama.org/_images/breakout.gif)\n",
    "\n",
    "Refer to <https://gymnasium.farama.org/environments/atari/breakout/> for more details.\n",
    "\n",
    "Gymnasium's Atari environment is used to simulate the game.\n",
    "The game's action space has 4 discrete actions: **NOOP**, **FIRE**, **RIGHT**, **LEFT**.\n",
    "(**NOOP** is the term for an action that has no effect, short for \"no-operation\".)\n",
    "The player starts each game (or each trajectory) with 5 lives, and loses a life whenever they miss an incoming ball.\n",
    "A new ball must be triggered with the **FIRE** action.\n",
    "The player gets one point per brick that they hit with the ball, after which this brick is destroyed.\n",
    "If the player loses all 5 lives, the game (and the trajectory) will be over.\n",
    "The final score for each game is the accumulated score of all 5 lives.\n",
    "\n",
    "You can play the game yourself by running the cell below! (Only works locally, not in Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not COLAB:\n",
    "    import gymnasium as gym\n",
    "    from gymnasium.utils.play import play\n",
    "\n",
    "    env = gym.make(\n",
    "        \"ALE/Breakout-v5\",\n",
    "        repeat_action_probability=0.0,  # deterministic mode\n",
    "        render_mode=\"rgb_array\",  # render to a numpy array for display with other tools\n",
    "    )\n",
    "    print(f\"Action meanings: {env.unwrapped.get_action_meanings()}\")\n",
    "    print(\n",
    "        \"Try Breakout yourself! Press the spacebar to start, and a/d to move the paddle left and right. Press Esc to exit.\"\n",
    "    )\n",
    "    play(\n",
    "        env,\n",
    "        keys_to_action={\n",
    "            \" \": 1,  # restart after lost life\n",
    "            \"d\": 2,  # right\n",
    "            \"a\": 3,  # left\n",
    "        },\n",
    "        noop=0,\n",
    "        fps=20,\n",
    "        zoom=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Overview\n",
    "\n",
    "The code below is organized into the following blocks.\n",
    "The bolded sections require your input:\n",
    " * Import statements and utility functions for plotting\n",
    " * Environment setup including Gymnasium wrappers\n",
    " * Hyperparameters\n",
    " * Updating the exploration rate, epsilon\n",
    " * The replay buffer\n",
    " * The QNetwork based on a CNN\n",
    " * **Action selection during policy rollout**\n",
    " * **Model optimization based on samples from the replay buffer**\n",
    " * **The main training loop, which coordinates rollouts, updating the replay buffer, and network updates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "enh5ZMHftEO7"
   },
   "outputs": [],
   "source": [
    "import random  # the standard library's random module is the best way to sample from a deque\n",
    "from collections import deque  # the replay buffer will be implemented as a deque\n",
    "from collections import namedtuple  # transitions will be stored as namedtuples\n",
    "import time\n",
    "from typing import Sequence\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "# Although we use Deep Learning, we still need to use some CPU memory for the\n",
    "# Replay Buffer, since Image data normally takes a lot of space and GPU memory\n",
    "# is more expensive than CPU memory\n",
    "import numpy as np\n",
    "\n",
    "# Deep Learning Platform\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 2\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Use GPU to speed up your training\n",
    "# detail: the code below will detect the hardware you have and set the\n",
    "# device to Nvidia \"cuda\" instead of \"cpu\".\n",
    "assert (\n",
    "    torch.cuda.is_available()\n",
    "), \"Change your Colab runtime to use a GPU, as training an image-based network takes forever on a CPU.\"\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "# Required to display matplotlib figures as cell output\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_rewards(num_games: Sequence[int], average_score: Sequence[float]) -> None:\n",
    "    \"\"\"\n",
    "    Plot average game score\n",
    "    Args:\n",
    "        num_games: x axis for different number of games\n",
    "        average_score: y axis for average game scores\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    display.clear_output(wait=True)\n",
    "    plt.plot(num_games, average_score)\n",
    "    plt.xlabel(\"Num of games\")\n",
    "    plt.ylabel(\"Mean game score\")\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Bj_5XyqHZVYZ"
   },
   "source": [
    "### Environment setup\n",
    "Create the environment and apply some wrappers to preprocess the data.\n",
    "On each step, the wrapper will return the last 4 grayscale screenshots as the observation (or state).\n",
    "For more preprocessing details, please refer to [Machado et al. (2018)](https://arxiv.org/abs/1709.06009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Dydj9zIzlIH"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import FrameStack, AtariPreprocessing\n",
    "from gymnasium.utils.save_video import save_video\n",
    "from stable_baselines3.common.atari_wrappers import FireResetEnv\n",
    "\n",
    "# create the environment, which has some wrappers by default\n",
    "env = gym.make(\n",
    "    \"ALE/Breakout-v5\",\n",
    "    frameskip=1,  # disable frameskip, as it's applied in AtariPreprocessing\n",
    "    repeat_action_probability=0.0,  # deterministic mode\n",
    "    render_mode=\"rgb_array_list\",  # render to a numpy array for saving with other tools\n",
    ")\n",
    "\n",
    "# apply typical preprocessing used for Atari environments, according to Machado et al. (2018)\n",
    "# Noop Reset: Obtains the initial state by taking a random number of no-ops on reset, default max 30 no-ops.\n",
    "# Max-pooling: Pools over the most recent two observations from the frame skips\n",
    "# Termination signal when a life is lost: When the agent loses a life during the environment, then the environment is terminated.\n",
    "# Resize to a square image: Resizes the atari environment original observation shape from 210x180 to 84x84 by default\n",
    "# Grayscale observation: If the observation is colour or greyscale, by default, greyscale.\n",
    "# We set frame_skip to 1 because frameskip is already applied by default\n",
    "env = AtariPreprocessing(env, frame_skip=4, terminal_on_life_loss=True)\n",
    "\n",
    "# Each time we reset the environment, press the \"FIRE\" button to launch the next ball\n",
    "env = FireResetEnv(env)\n",
    "\n",
    "# the agent observes the last 4 frames from the environment\n",
    "env = FrameStack(env, num_stack=4)\n",
    "\n",
    "# reset and apply random seed\n",
    "env.reset(seed=SEED)\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "\n",
    "# show the stack of wrappers and the underlying environment\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "n36K8CFIZVYZ"
   },
   "source": [
    "### Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "imnAkQ6jryL7"
   },
   "outputs": [],
   "source": [
    "# In Google Colab, this code block renders as a form where hyperparameters can be adjusted.\n",
    "\n",
    "# @markdown Number of transitions to sample from the replay buffer for each update:\n",
    "BATCH_SIZE = 32  # @param {type: \"integer\"}\n",
    "# @markdown Discount factor:\n",
    "GAMMA = 0.99  # @param {type: \"number\"}\n",
    "# @markdown Initial exploration rate (epsilon):\n",
    "EPS_START = 1.0  # @param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "# @markdown Final exploration rate (epsilon) after epsilon decay is complete:\n",
    "EPS_END = 0.05  # @param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "# @markdown Exploration rate (epsilon) decays linearly during initial fraction of training:\n",
    "EPS_FRACTION = 0.1  # @param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "# @markdown Target network is updated every N steps of the environment\n",
    "TARGET_UPDATE_INTERVAL = 10000  # @param {type: \"integer\"}\n",
    "\n",
    "# @markdown Learning rate (alpha)\n",
    "LEARNING_RATE = 1e-4  # @param {type: \"number\"}\n",
    "# @markdown Replay buffer size\n",
    "BUFFER_SIZE = 100000  # @param {type: \"integer\"}\n",
    "# @markdown Minimum number of environment samples collected before learning of the Q function begins\n",
    "LEARNING_STARTS = 10000  # @param {type: \"integer\"}\n",
    "# @markdown Total number of environments step for training\n",
    "TOTAL_STEPS = 500000  # @param {type: \"integer\"}\n",
    "\n",
    "# @markdown After backpropagation, clip the gradients if their total magnitude is greater than this:\n",
    "MAX_GRAD_NORM = 10  # @param {type: \"number\"}\n",
    "\n",
    "# @markdown Optimize the NN after this many rollouts:\n",
    "TRAIN_FREQ = 4  # @param {type: \"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "v3l3mzBjZVYZ"
   },
   "source": [
    "### Update exploration rate\n",
    "The exploration rate (epsilon) will decrease from 1 to 0.05 during the first 10% of training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWuhNkSArL1V"
   },
   "outputs": [],
   "source": [
    "def update_eps(total_time_steps: int) -> float:\n",
    "    \"\"\"\n",
    "    This is a helper function to update exploration rate, the exploration rate\n",
    "    will decrease from 1 to 0.05 during the first 10% of the training steps\n",
    "\n",
    "    Args:\n",
    "        total_time_steps: total time steps from the start of the training\n",
    "\n",
    "    Returns:\n",
    "        eps: exploration rate\n",
    "\n",
    "    \"\"\"\n",
    "    return max(\n",
    "        EPS_START - total_time_steps / (TOTAL_STEPS * EPS_FRACTION),\n",
    "        EPS_END,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "be9mXNKhZVYa"
   },
   "source": [
    "### Replay buffer\n",
    "\n",
    "The replay buffer stores the last N transitions, where a transition is a state, action, reward, the associated next state, as well as whether the trajectory terminated or truncated.\n",
    "Typically, the replay buffer stores the last 1 million transitions, we use 100K transitions for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "fLnU1evmss4I"
   },
   "outputs": [],
   "source": [
    "# Definition of transition stored by the replay buffer\n",
    "Transition = namedtuple(\n",
    "    \"Transition\",\n",
    "    (\"state\", \"action\", \"next_state\", \"reward\", \"terminated\", \"truncated\"),\n",
    ")\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Definition of reply buffer used by DQN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        \"\"\"Initialize the ReplayBuffer with certain capacity\"\"\"\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args: np.ndarray) -> None:\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "    ) -> tuple[\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "    ]:\n",
    "        \"\"\"\n",
    "        Sample transitions and transfer them from numpy array to PyTorch tensor\n",
    "        Args:\n",
    "            batch_size: mini batch size\n",
    "\n",
    "        Returns:\n",
    "            5 batches data\n",
    "        \"\"\"\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # a detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*random.choices(self.memory, k=batch_size)))\n",
    "\n",
    "        # prepare each element by concatenating, converting to tensor and pushing\n",
    "        # to device\n",
    "        terminated_b = torch.tensor(batch.terminated, device=device)\n",
    "        truncated_b = torch.tensor(batch.truncated, device=device)\n",
    "        state_b = torch.from_numpy(np.stack(batch.state)).to(device)\n",
    "        next_state_b = torch.from_numpy(np.stack(batch.next_state)).to(device)\n",
    "        action_b = torch.from_numpy(np.stack(batch.action)).to(device)\n",
    "        reward_b = torch.from_numpy(np.stack(batch.reward)).to(device, torch.float)\n",
    "        return state_b, action_b, next_state_b, reward_b, terminated_b, truncated_b\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Length of the ReplayBuffer\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "# Instantiate\n",
    "replay_buffer = ReplayBuffer(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXPHcsDrZVYa"
   },
   "source": [
    "### Deep Q-Network with CNN\n",
    "\n",
    "The following neural network is trained to represent the Q function, and implicitly represents the agent's policy as well.\n",
    "The input is the state `s`, and the output is a vector of the Q values for all possible actions in this state.\n",
    "Each state is a 3-dimensional Tensor of shape `(channel, height, width)`.\n",
    "During rollout, there is no batch dimension, as there is only one environment.\n",
    "Therefore, the input to the network is only 3-dimensional.\n",
    "During optimization, the network receives a batch of states sampled from the replay buffer, and the input is therefore 4-dimensional, with an extra batch dimension in the front.\n",
    "The underlying Pytorch code always expects 4-dimensional Tensors, but we must remove any batch dimensions we have added before returning.\n",
    "Images of uint8 type \\[0-255\\] are converted to floats and rescaled to \\[0-1\\) before being passed to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lj1WrUC9ZVYb"
   },
   "outputs": [],
   "source": [
    "class CnnQNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Q-Network with CNN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the network, which contains 2D convolution layers (image\n",
    "        process), Normalization layers (offer better numerical stability),\n",
    "        and Fully Connected layers\n",
    "        Args:\n",
    "            height: height of the image in pixel\n",
    "            width: width of the image in pixel\n",
    "            outputs: number of actions\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),  # flatten all convolutional features into one long vector\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, NUM_ACTIONS),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass function, given a state s, compute value of all actions\n",
    "        i.e. Q(s, a)\n",
    "        Called with either one element to determine next action, or a batch\n",
    "        during optimization.\n",
    "        Args:\n",
    "            x: state\n",
    "\n",
    "        Returns:\n",
    "            q_s_a = value of actions given this state\n",
    "        \"\"\"\n",
    "        # Shape of x:\n",
    "        # [num_frames=4, height=84, width=84] or [batch_size, num_frames=4, height=84, width=84]\n",
    "        #\n",
    "        # Shape of q_s_a:\n",
    "        # [NUM_ACTIONS=4] or [batch_size, NUM_ACTIONS=4]\n",
    "\n",
    "        if (ndim := x.ndim) == 3:\n",
    "            x = x.unsqueeze_(dim=0)  # add a batch dimension of size 1\n",
    "        else:\n",
    "            assert ndim == 4\n",
    "\n",
    "        # convert to float and rescale between [0-1), then do network forward pass\n",
    "        q_s_a = self.network(x / 255.0)\n",
    "\n",
    "        # restore original dimensionality\n",
    "        if ndim == 3:\n",
    "            q_s_a.squeeze_(dim=0)\n",
    "        return q_s_a\n",
    "\n",
    "\n",
    "# Instantiate policy net and its optimizer and push to GPU\n",
    "policy_net = CnnQNetwork().to(device)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Instantiate target net using NN parameters of the policy net\n",
    "target_net = CnnQNetwork().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.requires_grad_(False)  # Never record gradients with respect to these weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLORQjUQZVYb"
   },
   "source": [
    "## Sample action using epsilon-greedy exploration method (3pts)\n",
    "\n",
    "In the following block, you are going to select actions given a single **state** (3-dimensional Tensor) and an exploration rate **eps** (epsilon).\n",
    "Each action is an integer from the set {0, 1, 2, 3}.\n",
    "The program flow is:\n",
    "* Apply the epsilon-greedy method to decide if we go exploration or exploitation\n",
    "* If exploitation, use the policy net to compute the **Q(s,a)** and select the\n",
    " action with the highest value.\n",
    "* If exploration, randomly choose an action.\n",
    "\n",
    "Hints:\n",
    "* The epsilon-greedy method can be achieved by an if-statement comparing eps with a random float bounded by \\[0, 1\\), try `random.random()` to get a random number.\n",
    "* For exploitation:\n",
    "    * You do not need to track gradients when computing **Q(s,a)** (think about why!), so you should use a context scope `with torch.no_grad():`.\n",
    "    Disabling gradients when you don't need them always provides a significant speedup.\n",
    "    * You can apply **argmax()** to get the action, which is the index of the max **Q(s,a)**.\n",
    "* For exploration:\n",
    "    * Randomly picking actions can be achieved by generating a tensor of random integers.\n",
    "    `torch.randint()` is useful here, but be careful with the range and shape.\n",
    "    These integers will later be used to index PyTorch tensors, and must therefore be 64 bit integers.\n",
    "    Ensure this by passing `dtype` as `torch.long`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k465rsv0rL1X"
   },
   "outputs": [],
   "source": [
    "def select_action(state: torch.Tensor, eps: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        state: state tensor of shape [num_frames=4, height=84, width=84]\n",
    "        eps: exploration rate\n",
    "\n",
    "    Returns:\n",
    "        selected_action: Discrete action as a scalar (a tensor of dimension 0). The action is in the set {0, 1, 2, 3}\n",
    "\n",
    "    \"\"\"\n",
    "    ## TODO ##\n",
    "    selected_action = ...\n",    "\n",
    "    # Some code to help check the validity of the output\n",
    "    assert isinstance(selected_action, torch.Tensor)\n",
    "    assert selected_action.dtype == torch.long\n",
    "    assert selected_action.ndim == 0\n",
    "\n",
    "    return selected_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfd3KIAlZVYb"
   },
   "source": [
    "## Optimize the model (7pts)\n",
    "\n",
    "In the following block, you are going to compute a loss which will be used to optimize the parameters of the network.\n",
    "\n",
    "The workflow is:\n",
    "* Define a loss function (use the Huber loss).\n",
    "* Sample a mini batch of transitions from the replay buffer.\n",
    "* Query the policy network for Q values for the states.\n",
    "* Choose the Q values corresponding to the actions that were taken in the sampled transitions.\n",
    "* Query the target network for Q values for the next states.\n",
    "* Choose the maximum Q value over all actions for each next state.\n",
    "* Compute targets with $r + \\gamma~\\max_{a'} Q(s', a')$.\n",
    "Take care to only add the max Q of the next state, if the transition was not terminal!\n",
    "* Use the loss function to compute the Huber loss.\n",
    "* Carry out back propagation to get the gradients, clip them, and step the optimizer.\n",
    "\n",
    "Hints:\n",
    "* You can use `torch.logical_not` or the `~` operator to invert a Tensor of boolean values.\n",
    "* To pick data from tensor A using tensor B as index, use `torch.gather`.\n",
    "* To add an extra dimension to a Tensor after dimension `i`, use `torch.unsqueeze(dim=i)`. The new dimension will have a length of 1.\n",
    "* To remove dimension `i` from a Tensor, use `torch.squeeze(dim=i)`. This only works dimension `i` has a length of 1.\n",
    "* To get maximum values of a Tensor along some axis, use `torch.max`.\n",
    "* Multiplying a Tensor of booleans with a Tensor of floats does what you expect and may be useful.\n",
    "* Once you instantiate a loss function (.e.g `nn.HuberLoss()`), you can call that instance to compute a loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rgiU2d1OrL1X"
   },
   "outputs": [],
   "source": [
    "## TODO ##\n",
    "loss_func = ...\n",    "\n",
    "\n",
    "def optimize_model():\n",
    "    # Do not train until we have enough data in the buffer\n",
    "    if len(replay_buffer) < LEARNING_STARTS:\n",
    "        return\n",
    "\n",
    "    # Sample mini-batch of transitions\n",
    "    (\n",
    "        states,\n",
    "        actions,\n",
    "        next_states,\n",
    "        rewards,\n",
    "        terminateds,\n",
    "        truncateds,\n",
    "    ) = replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "    ## TODO ##\n",
    "    loss = ...\n",    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), MAX_GRAD_NORM)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCwXxx7VZVYc"
   },
   "source": [
    "### Main loop (5pts)\n",
    "\n",
    "In the following block, you are going to finish the main loop of the training procedure.\n",
    "\n",
    "The program flow is:\n",
    "* Firstly initialize variables for program state, like game score, number of games, total steps, etc.\n",
    "* Start the game to get its first state\n",
    "* While loop until max training steps has been reached:\n",
    "    * While loop until **terminated or truncated** (usually because a life was lost):\n",
    "        * Update exploration rate\n",
    "        * Apply a `torch.no_grad()` context so that gradients are not recorded for rollouts:\n",
    "            * Convert state from numpy array to torch Tensor and push to GPU\n",
    "            * Select action\n",
    "            * Push action to CPU and convert to numpy array\n",
    "            * Execute action in the environment, receiving a tuple of results, which includes the next state\n",
    "            * Save transition into replay buffer\n",
    "            * Move to the next state\n",
    "            * Update counters\n",
    "        * Optimize policy net when necessary\n",
    "        * Update target net when necessary\n",
    "    * Update progress bar, plot of running average game score\n",
    "    * Save video of policy rollout\n",
    "    * Reset environment to prepare for next trajectory\n",
    "\n",
    "Hint:\n",
    "* To convert a numpy array to torch, use `torch.from_numpy`.\n",
    "* A torch Tensor can be pushed to the CPU using `x.cpu()` or to the GPU using `x.to(device)`.\n",
    "* A torch Tensor **on the CPU** can be converted to a numpy array using `x.numpy()`.\n",
    "* Use `env.step(action)`to perform an action.\n",
    "This function returns a tuple of 5 values: `(next_state, reward, terminated, truncated, info)`.\n",
    "The info is not relevant for this exercise.\n",
    "\n",
    "When you start training, the early stage will be quite noisy.\n",
    "**You may need 20+ min to get the average score more than 1**.\n",
    "The entire training should take about 45 minutes, and the average score at the end should be around 2-3.\n",
    "Every 250 episodes, a video of the policy is saved (either to Google Drive or locally).\n",
    "You can use this to visually confirm that the policy is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "_qQb789_syt0",
    "outputId": "4fdd1b8e-8a88-4c68-e3b6-9586ca89d718"
   },
   "outputs": [],
   "source": [
    "list_num_game = []\n",
    "list_mean_game_scores = []\n",
    "\n",
    "# Main Loop of training\n",
    "state, _ = env.reset(seed=2)\n",
    "terminated, truncated = False, False\n",
    "eps = EPS_START\n",
    "num_time_steps = 0\n",
    "num_game = 0\n",
    "game_score = 0\n",
    "game_length = 0\n",
    "game_scores = deque([], maxlen=10)  # Store the score of the latest 10 games\n",
    "video_folder = DATA_ROOT / \"exercise_2\" / time.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "# Progress bar\n",
    "with tqdm(total=TOTAL_STEPS, position=0, leave=True, unit=\"steps\") as pbar:\n",
    "    # Loop until total time steps has been reached\n",
    "    while num_time_steps < TOTAL_STEPS:\n",
    "        # Loop until trajectory is over\n",
    "        while not (terminated or truncated):\n",
    "            # Get exploration rate\n",
    "            eps = update_eps(num_time_steps)\n",
    "\n",
    "            # Rollout\n",
    "            with torch.no_grad():\n",
    "                ## TODO ##\n",    "\n",
    "                # Update game score and length and total time steps\n",
    "                game_score += reward\n",
    "                game_length += 1\n",
    "                num_time_steps += 1\n",
    "\n",
    "            # Optimize model\n",
    "            if num_time_steps % TRAIN_FREQ == 0:\n",
    "                optimize_model()\n",
    "\n",
    "            # Update the target network\n",
    "            if num_time_steps % TARGET_UPDATE_INTERVAL == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        # record game score and length\n",
    "        game_scores.append(game_score)\n",
    "        mean_game_score = np.asarray(game_scores).mean().item()\n",
    "        pbar.update(game_length)\n",
    "        num_game += 1\n",
    "\n",
    "        # Reset game score and length\n",
    "        game_score = 0\n",
    "        game_length = 0\n",
    "\n",
    "        # Print some result in the progress bar for every 10 games\n",
    "        if num_game % 10 == 0:\n",
    "            pbar.set_description(\n",
    "                f\"Game #{num_game - 9}-{num_game}, Avg_score: {mean_game_score:.3f},\"\n",
    "                f\" eps: {eps:.3f}\"\n",
    "            )\n",
    "\n",
    "        # Plot the average reward curve for every 50 games\n",
    "        if num_game % 50 == 0:\n",
    "            list_num_game.append(num_game)\n",
    "            list_mean_game_scores.append(mean_game_score)\n",
    "            plot_rewards(list_num_game, list_mean_game_scores)\n",
    "\n",
    "        # save video of last game, only every 250 games\n",
    "        save_video(\n",
    "            env.render(),\n",
    "            video_folder=video_folder,\n",
    "            episode_trigger=lambda x: x % 250 == 0,\n",
    "            name_prefix=\"breakout\",\n",
    "            episode_index=num_game,\n",
    "            fps=30,\n",
    "        )\n",
    "\n",
    "        # reset environment for next trajectory\n",
    "        state, _ = env.reset()\n",
    "        terminated, truncated = False, False\n",
    "\n",
    "    pbar.close()\n",
    "    print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Self-test questions (optional)\n",
    "\n",
    "Where and how can we change the model to get a Double DQN?\n",
    "\n",
    ".## TODO ##\n",
    "\n",    "\n",
    "\n",
    "After training on Breakout for 500000 steps, the agent manages to break a handful of squares each game.\n",
    "But in the lecture, it said that DQN can achieve superhuman performance on Atari?\n",
    "Why is our agent so bad?\n",
    "\n",
    ".## TODO ##\n",
    "\n",    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "eQx7oDGeeKWj"
   ],
   "name": "2_dqn_atari.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
