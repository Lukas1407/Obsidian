{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Setup\n",
    "\n",
    "If you prefer to work locally, see the following instructions for setting up Python in a virtual environment.\n",
    "You can then ignore the instructions in \"Colab Setup\".\n",
    "\n",
    "If you haven't yet, create a [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html) environment using:\n",
    "```\n",
    "conda create --name rl_exercises\n",
    "conda activate rl_exercises\n",
    "```\n",
    "Torch recommends installation using conda rather than pip, so run:\n",
    "```\n",
    "conda install pytorch cpuonly -c pytorch\n",
    "```\n",
    "If you have a CUDA-enabled GPU and would like to use it, visit [the installation page](https://pytorch.org/get-started/locally/) to see the options available for different CUDA versions.\n",
    "The remaining dependencies can be installed with pip:\n",
    "```\n",
    "pip install matplotlib numpy tqdm ipykernel \"gymnasium[toy-text]\"\n",
    "```\n",
    "\n",
    "Even if you are running the Jupyter notebook locally, please run the code cells in **Colab Setup**, because they define some global variables required later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Colab Setup\n",
    "\n",
    "Google Colab provides you with a temporary environment for python programming.\n",
    "While this conveniently works on any platform and internally handles dependency issues and such, it also requires you to set up the environment from scratch every time.\n",
    "The \"Colab Setup\" section below will be part of **every** exercise and contains utility that is needed before getting started.\n",
    "\n",
    "There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window).\n",
    "Any changes you make to the Jupyter notebook itself should be saved to your Google Drive.\n",
    "We also save all recordings and logs in it by default so that you won't lose your work in the event of an instance timeout.\n",
    "However, you will need to re-mount your Google Drive and re-install packages with every new instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Your work will be stored in a folder called `rl_ws23` by default to prevent Colab \n",
    "instance timeouts from deleting your edits.\n",
    "We do this by mounting your google drive on the virtual machine created in this colab \n",
    "session. For this, you will likely need to sign in to your Google account and allow\n",
    "access to your Google Drive files.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    COLAB = True\n",
    "except ImportError:\n",
    "    COLAB = False\n",
    "\n",
    "# Create paths in your google drive\n",
    "if COLAB:\n",
    "    DRIVE_PATH = \"/content/gdrive/My\\ Drive/rl_ws23\"\n",
    "    DRIVE_PYTHON_PATH = DRIVE_PATH.replace(\"\\\\\", \"\")\n",
    "    if not os.path.exists(DRIVE_PYTHON_PATH):\n",
    "        %mkdir $DRIVE_PATH\n",
    "\n",
    "    # the space in `My Drive` causes some issues,\n",
    "    # make a symlink to avoid this\n",
    "    DATA_ROOT = \"/content/rl_ws23\"\n",
    "    if not os.path.exists(DATA_ROOT):\n",
    "        !ln -s $DRIVE_PATH $DATA_ROOT\n",
    "    %cd $DATA_ROOT\n",
    "\n",
    "    DATA_ROOT = Path(DATA_ROOT)\n",
    "else:\n",
    "    DATA_ROOT = Path.cwd() / \"rl_ws23\"\n",
    "\n",
    "# Install **python** packages\n",
    "if COLAB:\n",
    "    %pip install matplotlib numpy tqdm gymnasium[toy-text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "In this homework, we will implement basic planning and reinforcement learning algorithms.\n",
    "We will look at Policy Iteration and Value Iteration, as well as tabular Q-Learning.\n",
    "The algorithms will be evaluated on a gridworld task from OpenAI gym.\n",
    "\n",
    "All homeworks are self-contained.\n",
    "They can be completed in their respective notebooks.\n",
    "To edit and re-run code, you can therefore simply edit and restart the code cells below.\n",
    "\n",
    "We start by importing all the necessary python modules and defining some helper functions which you do not need to\n",
    "change.\n",
    "Still, make sure you are aware of what they do.\n",
    "The code is split into the following sections:\n",
    " * Logging/Recording utility\n",
    " * Runtime arguments\n",
    " * Functional code (classes, functions, ...) and explanations\n",
    "\n",
    "## Hint\n",
    "\n",
    "We will be working with Gymnasium DiscreteEnvs. These environments have a property P which is a dictionary of lists, where\n",
    "```\n",
    "P[s][a] == [(transition probability, next state, reward, done), ...]\n",
    "```\n",
    "for a state $s$ and an action $a$. In FrozenLake, each action has a 33% chance of being executed correctly, and a 33%\n",
    "chance for adjacent actions, respectively. In non-terminal states, len(P[s][a]) == 3.\n",
    "This environment is very similar to the one shown in the Optimal Decision Making lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "\n",
    "class ProgressBar:\n",
    "    def __init__(self, num_iterations: int, verbose: bool = True):\n",
    "        if verbose:  # create a nice little progress bar\n",
    "            self.scalar_tracker = tqdm.tqdm(\n",
    "                total=num_iterations,\n",
    "                desc=\"Scalars\",\n",
    "                bar_format=\"{desc}\",\n",
    "                position=0,\n",
    "                leave=True,\n",
    "            )\n",
    "            progress_bar_format = (\n",
    "                \"{desc} {n_fmt:\"\n",
    "                + str(len(str(num_iterations)))\n",
    "                + \"}/{total_fmt}|{bar}|{elapsed}<{remaining}\"\n",
    "            )\n",
    "            self.progress_bar = tqdm.tqdm(\n",
    "                total=num_iterations,\n",
    "                desc=\"Iteration\",\n",
    "                bar_format=progress_bar_format,\n",
    "                position=1,\n",
    "                leave=True,\n",
    "            )\n",
    "            self.verbose = True\n",
    "        else:\n",
    "            self.verbose = False\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        if self.verbose:\n",
    "            formatted_scalars = {\n",
    "                key: \"{:.3e}\".format(value[-1] if isinstance(value, list) else value)\n",
    "                for key, value in kwargs.items()\n",
    "            }\n",
    "            description = \"Scalars: \" + \", \".join(\n",
    "                [f\"{key}={value}\" for key, value in formatted_scalars.items()]\n",
    "            )\n",
    "            self.scalar_tracker.set_description(description)\n",
    "            self.progress_bar.update(1)\n",
    "\n",
    "\n",
    "# specify the path to save the recordings of this run to.\n",
    "DATA_PATH = DATA_ROOT / \"exercise_1\" / time.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "# this function will automatically save the current figure into your google drive folder or local directory\n",
    "def save_figure(save_name: str) -> None:\n",
    "    DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    path = DATA_PATH / (save_name + \".png\")\n",
    "    plt.savefig(str(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym  # gymnasium is successor to gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.99  # discount factor\n",
    "alpha = 0.1  # learning rate\n",
    "t_decay = 10000  # rate at which greedy actions are preferred over random ones\n",
    "\n",
    "\n",
    "def measure_policy_success(\n",
    "    env: FrozenLakeEnv,\n",
    "    pi: np.ndarray,\n",
    "    n_eval: int,\n",
    "    render: bool = False,\n",
    ") -> float:\n",
    "    \"\"\"Evaluate a policy on an environment and return success rate\n",
    "\n",
    "    :param env: FrozenLake env\n",
    "    :param pi: a policy represented by action probabilities for each action in each state\n",
    "    :param render: Render the policy\n",
    "    :return: The mean success rate of the given policy\n",
    "    \"\"\"\n",
    "\n",
    "    successes = []\n",
    "    for _ in range(n_eval):\n",
    "        state, info = env.reset()\n",
    "        if render:\n",
    "            window = plt.imshow(env.render())\n",
    "        for _ in range(100):\n",
    "            action = int(pi[state])\n",
    "\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            if render:\n",
    "                window.set_data(env.render())\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                successes.append(reward)\n",
    "                break\n",
    "\n",
    "    return np.mean(successes)\n",
    "\n",
    "\n",
    "def plot_learning_curve(success_rates: list[float]) -> None:\n",
    "    \"\"\"Plot the learning curve\"\"\"\n",
    "    _, ax = plt.subplots()\n",
    "    ax.plot(success_rates)\n",
    "    ax.set_xlabel(\"Iterations\")\n",
    "    ax.set_ylabel(\"Success Rate\")\n",
    "\n",
    "\n",
    "def plot_value_function(value_function: np.ndarray, env: FrozenLakeEnv) -> None:\n",
    "    \"\"\"Plot the value function for a given environment.\n",
    "\n",
    "    :param vf: A value function\n",
    "    :param env: An environment\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    shape = (env.unwrapped.nrow, env.unwrapped.ncol)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(value_function.reshape(shape))\n",
    "\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            text = ax.text(\n",
    "                j,\n",
    "                i,\n",
    "                env.unwrapped.desc[i, j].decode(),\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                color=\"w\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The FrozenLake Environment\n",
    "First, let's have a look at the problem we are solving. The agent controls the movement of a character in a grid world.\n",
    "Some tiles of the grid are walkable, and others lead to the agent falling into the water.\n",
    "Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction.\n",
    "The agent is rewarded for finding a walkable path to a goal tile.\n",
    "The episode ends when you reach the goal or fall in a hole.\n",
    "You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "Actions are encoded as integers 0 = left, 1 = down, 2 = right and 3 = up.\n",
    "The states are counted from 0 to $N_{states}$.\n",
    "\n",
    "![The FrozenLake Environment](https://gymnasium.farama.org/_images/frozen_lake.gif)\n",
    "\n",
    "See https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
    "\n",
    "Execute the next cell to see what a randomly initialized policy does.\n",
    "\n",
    "Note: the dynamics of the environment are available to you through the `mdp` variable.\n",
    "If you index this with the state and the action (i.e. `mdp[state][action]`), you are given a list of possible transitions, where each transition is a tuple of the form `(transition_prob: float, next_state: int, reward: float, done: bool)`.\n",
    "In other words, each transition has an associated probability, reward, and next state.\n",
    "Because the reward depends on the next state and not just the action (i.e. $r=r(s,a,s')$), we must slightly update the equations for policy iteration and value iteration seen in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "env: FrozenLakeEnv = gym.make(\"FrozenLake-v1\", map_name=\"8x8\", render_mode=\"rgb_array\")\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n\n",
    "pi_random = np.random.choice(n_actions, n_states)\n",
    "\n",
    "success_rate = measure_policy_success(env, pi_random, n_eval=1, render=True)\n",
    "\n",
    "print(f\"Random policy success rate={success_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TASK 1: Policy Iteration**\n",
    "The first algorithm we will implement is called Policy Iteration.\n",
    "It consists of an inner and an outer loop.\n",
    "The inner loop is called Policy Evaluation and evaluates a given policy on a problem.\n",
    "For every state, it returns the exact value, i.e. the expected return under the current policy.\n",
    "The outer loop is called Policy Improvement and, as the name suggests, takes the current policy's value function\n",
    "and returns an improved version.\n",
    "In Policy Iteration, we first initialize a policy and value function randomly, and then iteratively run Policy\n",
    "Evaluation and Policy Improvement until convergence.\n",
    "\n",
    "The **pseudocode** looks as follows:\n",
    "\n",
    "---\n",
    "- **Init** Initialize $V_{(0)}^{\\pi_0}(s)=0$ for all $s$, $\\pi_0 \\leftarrow$ uniform, $k = 0$\n",
    "\n",
    "- **Repeat**  For $i=1, 2, \\dots$\n",
    "\n",
    "    - `Policy Evaluation`\n",
    "\n",
    "        - **Init** Initialize $V_{(0)}^{\\pi_{i}}(s) = V_{(k)}^{\\pi_{i-1}}(s)$ (i.e. initialize the value function of the new policy with the converged value function of the old policy)\n",
    "\n",
    "        - **Repeat** For $k=1, 2, \\dots$\n",
    "\n",
    "            \\begin{equation*}\n",
    "                V_{(k)}^{\\pi_{i}}(s) = \\sum_a \\pi(a \\mid s) \\sum_{s'} P(s' \\mid s,a) \\Big( r(s,a,s') + \\gamma \\, V_{(k - 1)}^{\\pi_{i}}(s') \\Big)\n",
    "            \\end{equation*}\n",
    "\n",
    "        - **Until convergence**\n",
    "\n",
    "    - `Policy Improvement`\n",
    "\n",
    "        \\begin{align*}\n",
    "            \\pi_{(i+1)}(a \\mid s) = \\begin{cases}\n",
    "                1, & \\text{if } a =  \\underset{a'}{\\arg \\max} \\sum_{s'} P(s' \\mid s,a') \\Big( r(s,a',s') + \\gamma \\, V^{\\pi_i}(s') \\Big)\\\\\n",
    "                0 & \\text{else}\n",
    "            \\end{cases}\n",
    "        \\end{align*}\n",
    "\n",
    "- **Until convergence**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1 (4 Points)\n",
    "In this first task you have to implement the missing code snippets (marked as *TODO*) of the policy iteration class. The **pseudocode** shown above might be helpful. \n",
    "1. Implement the missing code snippet of the **policy_evaluation** function below. (2 points)\n",
    "2. Implement the missing code snippet of the **policy_improvement** function below. (2 points)\n",
    "\n",
    "After you have finished implementing the corresponding functions, execute the code cell *Run Policy Iteration*. This code cell will automatically save your value function plot and the learning curve plot into your mounted drive folder. You will need to submit these figures together with the notebook as a zip file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    def __init__(\n",
    "        self,\n",
    "        gamma: float,\n",
    "        n_iters: int,\n",
    "        policy_eval_iters: int = 10000,\n",
    "    ) -> None:\n",
    "        \"\"\"A class for performing Policy Iteration\n",
    "\n",
    "        :param env:\n",
    "        :param gamma:\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.n_iters = n_iters\n",
    "        self.policy_eval_iters = policy_eval_iters\n",
    "\n",
    "    def policy_evaluation(\n",
    "        self,\n",
    "        mdp: dict[int, dict[int, list[tuple[float, int, float, bool]]]],\n",
    "        pi_prob: np.ndarray,\n",
    "        previous_value: np.ndarray,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Perform the Policy Evaluation step given a policy pi and an environment.\n",
    "\n",
    "        :param mdp: an MDP as a list of transitions for each state-action pair\n",
    "        :param pi_prob: Action probabilities [num_states, num_actions]\n",
    "        :param previous_value: Initial value function\n",
    "        :return: value function of the provided policy [num_states]\n",
    "        \"\"\"\n",
    "        n_states, n_actions = pi_prob.shape\n",
    "        value = np.copy(previous_value)\n",
    "        for _ in range(self.policy_eval_iters):\n",
    "            ## TODO ##\n",
    "            # your code here\n",
    "            # hint: you will need to iterate over states and actions here\n",
    "            # get a list of possible transitions using mdp[state][action], which returns a list of\n",
    "            # tuples of (transition_prob, next_state, reward, done)\n",    "\n",
    "            # run policy evaluation until convergence\n",
    "            if np.allclose(value, previous_value):\n",
    "                break\n",
    "\n",
    "            # save current value estimate for next iteration\n",
    "            previous_value[...] = value\n",
    "\n",
    "        return value\n",
    "\n",
    "    def policy_improvement(\n",
    "        self,\n",
    "        mdp: dict[int, dict[int, list[tuple[float, int, float, bool]]]],\n",
    "        value: np.ndarray,\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Perform the Policy Improvement step given a value function.\n",
    "\n",
    "        :param mdp: an MDP as a list of transitions for each state-action pair\n",
    "        :param value: Value function of a policy [num_states]\n",
    "        :return: New policy [num_states] and distribution over action probabilities [num_states, num_actions]\n",
    "        \"\"\"\n",
    "        n_states, n_actions = len(mdp), len(mdp[0])\n",
    "        # initialize policy\n",
    "        # contains the actual actions\n",
    "        pi = np.zeros(shape=n_states, dtype=np.int64)\n",
    "        # contains the action probabilities for each state\n",
    "        pi_prob = np.zeros(shape=(n_states, n_actions))\n",
    "\n",
    "        ## TODO ##\n",
    "        # your code here\n",
    "        # hint: This again requires you to iterate over states and actions in some way.\n",
    "        # get a list of possible transitions using mdp[state][action], which returns a list of\n",
    "        # tuples of (transition_prob, next_state, reward, done)\n",
    "        # You can use np.argmax() to get the index of the biggest value in an array.\n",    "\n",
    "        return pi, pi_prob\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        env: FrozenLakeEnv,\n",
    "    ) -> tuple[np.ndarray, np.ndarray, list[float]]:\n",
    "        \"\"\"Perform Policy Iteration given a DiscreteEnv environment and a discount factor gamma for n_iter iterations\n",
    "\n",
    "        :param env: An openAI Gym DiscreteEnv object\n",
    "        :param gamma: Discount factor\n",
    "        :param n_iter: Number of PI iterations\n",
    "        :return: Final value function [num_states] and optimal policy [num_states]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        n_states = env.observation_space.n\n",
    "        n_actions = env.action_space.n\n",
    "        mdp = env.unwrapped.P\n",
    "        value = np.zeros(shape=n_states)\n",
    "        pi = np.zeros(shape=n_states, dtype=np.int64)  # contains the actual actions\n",
    "        pi_prob = (\n",
    "            np.ones(shape=(n_states, n_actions)) / n_actions\n",
    "        )  # contains the action probabilities for each state\n",
    "\n",
    "        print(\"iter   | max|V-Vprev| | #actions chg | V[0]  \")\n",
    "        print(\"-------+--------------+--------------+-------\")\n",
    "        success_rates = []\n",
    "        for iter in range(self.n_iters):\n",
    "            previous_pi = np.copy(pi)\n",
    "            previous_value = np.copy(value)\n",
    "\n",
    "            # run policy evaluation\n",
    "            value = self.policy_evaluation(mdp, pi_prob, value)\n",
    "\n",
    "            # run policy improvement\n",
    "            pi, pi_prob = self.policy_improvement(mdp, value)\n",
    "\n",
    "            # evaluate policy success rate\n",
    "            success_rate = measure_policy_success(env, pi, n_eval=100)\n",
    "            success_rates.append(success_rate)\n",
    "\n",
    "            max_diff = np.abs(value - previous_value).max()\n",
    "            n_chg_actions = (pi != previous_pi).sum()\n",
    "\n",
    "            print(\n",
    "                f\"{iter:4d}   | {max_diff:12.5f} |   {n_chg_actions:4d}       | {value[0]:7.3f}\"\n",
    "            )\n",
    "\n",
    "        return value, pi, success_rates\n",
    "\n",
    "\n",
    "# Run Policy Iteration\n",
    "\n",
    "env: FrozenLakeEnv = gym.make(\"FrozenLake-v1\", map_name=\"8x8\")\n",
    "\n",
    "policy_iteration = PolicyIteration(\n",
    "    gamma,\n",
    "    n_iters=10,\n",
    ")\n",
    "value, pi, success_rates = policy_iteration(env)\n",
    "\n",
    "plot_value_function(value, env)\n",
    "save_figure(\"vf_pi\")\n",
    "plot_learning_curve(success_rates)\n",
    "save_figure(\"lc_pi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TASK 2: Value Iteration**\n",
    "Next, we will have a look at Value Iteration.\n",
    "It is very similar to Policy Iteration and only executes one step of policy evaluation.\n",
    "The **pseudocode** looks as follows\n",
    "\n",
    "---\n",
    "- **Init** Initialize $V_{(0)}^\\ast(s)=0$, for all $s$\n",
    "\n",
    "- **Repeat**  For $i=1, 2, \\dots$\n",
    "\n",
    "    \\begin{equation*}\n",
    "        V^\\ast_{(i)}(s) = \\underset{a}{\\max} \\sum_{s'} P(s' \\mid s,a) \\Big( r(s,a,s') + \\gamma \\, V^\\ast_{(i-1)}(s') \\Big)\n",
    "    \\end{equation*}\n",
    "\n",
    "- **Until convergence**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 (4 Points)\n",
    "In this task you have to implement the missing code snippets (marked as *TODO*) of the Value iteration calss. The **psuedocode** shown above might be helpful.\n",
    "1. Implement the missing code snippet of the **value_iteration** function below \n",
    "\n",
    "After you have finished implementing the corresponding functions, execute the code cell *Run Value Iteration*. This code cell will automatically save your value function plot and the learning curve plot into your mounted drive folder. You will need to submit these figures together with the notebook as a zip file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class ValueIteration:\n",
    "    def __init__(self, gamma: float, n_iters: int) -> None:\n",
    "        \"\"\"A class for performing Value Iteration\n",
    "\n",
    "        :param env: A FrozenLakeEnv environment\n",
    "        :param gamma: Discount factor\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.n_iters = n_iters\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        env: FrozenLakeEnv,\n",
    "    ) -> tuple[np.ndarray, np.ndarray, list[float]]:\n",
    "        \"\"\"Perform Value Iteration given a DiscreteEnv environment and a discount factor gamma for n_iter iterations.\n",
    "\n",
    "        :param n_iter: Number of VI iterations\n",
    "        :return: A tuple (final value_function [num_states], optimal policy [num_states])\n",
    "        \"\"\"\n",
    "\n",
    "        n_states = env.observation_space.n\n",
    "        n_actions = env.action_space.n\n",
    "        mdp = env.unwrapped.P\n",
    "        value = np.zeros(shape=n_states)\n",
    "        pi = np.zeros(shape=n_states, dtype=np.int64)  # contains the actual actions\n",
    "\n",
    "        print(\"iter   | max|V-Vprev| | #actions chg | V[0]  \")\n",
    "        print(\"-------+--------------+--------------+-------\")\n",
    "        success_rates = []\n",
    "        for iter in range(0, self.n_iters):\n",
    "            previous_pi = np.copy(pi)\n",
    "            previous_value = np.copy(value)\n",
    "\n",
    "            ## TODO ##\n",
    "            # your code here\n",
    "            # hint: Here, you will need to fill in the new policy pi and value function v for all states s.\n",
    "            # I.e., you need to update pi[s], v[s] for all s.\n",
    "            # get a list of possible transitions using mdp[state][action], which returns a list of\n",
    "            # tuples of (transition_prob, next_state, reward, done)\n",    "\n",
    "            # Evaluate policy success rate\n",
    "            success_rate = measure_policy_success(env, pi, n_eval=100)\n",
    "            success_rates.append(success_rate)\n",
    "\n",
    "            max_diff = np.abs(value - previous_value).max()\n",
    "            n_chg_actions = (pi != previous_pi).sum()\n",
    "\n",
    "            print(\n",
    "                f\"{iter:4d}   | {max_diff:12.5f} |   {n_chg_actions:4d}       | {value[0]:7.3f}\"\n",
    "            )\n",
    "\n",
    "        return value, pi, success_rates\n",
    "\n",
    "\n",
    "# Run Value Iteration\n",
    "\n",
    "env: FrozenLakeEnv = gym.make(\"FrozenLake-v1\", map_name=\"8x8\")\n",
    "\n",
    "value_iteration = ValueIteration(gamma, n_iters=100)\n",
    "value, pi, success_rates = value_iteration(env)\n",
    "\n",
    "plot_value_function(value, env)\n",
    "save_figure(\"vf_vi\")\n",
    "plot_learning_curve(success_rates)\n",
    "save_figure(\"lc_vi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TASK 3: Q-Learning**\n",
    "In Policy Iteration and Value Iteration, we assume to have knowledge about the problem's underlying dynamics and reward\n",
    "function. These properties are usually not available in practice.\n",
    "Instead, we need to solve the problem by exploiting what we have learned so far and exploring previously unseen situations.\n",
    "An algorithm to solve a problem in this fashion is Q-Learning. The **pseudocode** is given as\n",
    "\n",
    "---\n",
    "- **Init** Initialize $Q_{(0)}(s, a)=0$, for all $s$ and $a$\n",
    "\n",
    "- **Repeat**  For $i=1, 2, \\dots$\n",
    "    - sample an action $a$ using the exploration strategy and get the next state $s'$ and associated reward $r$\n",
    "    - If s' is terminal:\n",
    "        - $\\delta = r(s, a) - Q_{(i-1)}(s, a)$\n",
    "        - Reset environment and sample new initial state $ s' $\n",
    "    - Else:\n",
    "        - $\\delta = r(s, a) + \\gamma \\underset{a'}{\\max} Q_{(i-1)}(s', a') - Q_{(i-1)}(s, a)$\n",
    "\n",
    "    $Q_{(i)}(s, a) = Q_{(i-1)}(s, a) + \\alpha \\delta$ \n",
    "    \n",
    "    Set $ s \\leftarrow s' $\n",
    "\n",
    "- **Until convergence**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 (4 Points)\n",
    "We will implement the missing cod snippet (marked as *TODO*) of the QLearning class. The **pseudoclass** shown above might be helpful.\n",
    "\n",
    "1. Finish the Q-Learning inner loop in the learn function, by implementing the TD-Error and the Q-Learning update rule \n",
    "\n",
    "After you have finished implementing the corresponding functions, execute the code cell *Run QLearning*. This code cell will automatically save your value function plot and the learning curve plot into your mounted drive folder. You will need to submit these figures together with the notebook as a zip file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(\n",
    "        self,\n",
    "        gamma: float,\n",
    "        alpha: float,\n",
    "        t_decay: float,\n",
    "        n_iters: int,\n",
    "    ) -> None:\n",
    "        \"\"\"A class for tabular Q-Learning with epsilon-greedy exploration.\n",
    "         The policy is defined implicitly by the Q-function attribute.\n",
    "\n",
    "        :param env: A FrozenLakeEnv environment\n",
    "        :param gamma: Discount factor\n",
    "        :param alpha: Learning rate\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.t_decay = t_decay\n",
    "        self.n_iters = n_iters\n",
    "\n",
    "    def sample_action(self, q: np.ndarray, state: int, t: int) -> int:\n",
    "        \"\"\"Sample an action based given the state id on the epsilon greedy strategy with time decaying exploration.\n",
    "\n",
    "        :param s: A state id represented as an integer over all state ids.\n",
    "        :return: An action id, corresponding to the available actions described above.\n",
    "        \"\"\"\n",
    "\n",
    "        if np.random.random() < min(0.99, t / self.t_decay):\n",
    "            # return the greedy action, i.e. the one with the highest q\n",
    "            action = int(np.argmax(q[state]))\n",
    "        else:\n",
    "            # return a random action\n",
    "            action = np.random.randint(0, len(q[state]))\n",
    "\n",
    "        return action\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        env: FrozenLakeEnv,\n",
    "    ) -> tuple[np.ndarray, np.ndarray, list[float]]:\n",
    "        \"\"\"Perform n_iter iterations of Q-Learning\n",
    "\n",
    "        :param n_iter: Number of learning iterations\n",
    "        :return: The learned Value function and policy as a tuple (final value_function [num_states], optimal policy [num_states])\n",
    "        \"\"\"\n",
    "        n_states = env.observation_space.n\n",
    "        n_actions = env.action_space.n\n",
    "        q = np.zeros((n_states, n_actions))\n",
    "\n",
    "        state, info = env.reset()\n",
    "\n",
    "        success_rates = []\n",
    "        for t in range(self.n_iters):\n",
    "            ## TODO ##\n",
    "            action = ...\n",    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            ## TODO ##\n",
    "            # your code here\n",
    "            # hint: Here, you will need to update the Q function with the data from the\n",
    "            # latest transition above\n",
    "            # hint: use an if-else to handle terminal states (terminated==True) differently from non-terminal ones.\n",    "\n",
    "            if terminated or truncated:\n",
    "                state, info = env.reset()\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "            # evaluate policy success rate\n",
    "            if t % 100 == 0:\n",
    "                success_rate = measure_policy_success(env, np.argmax(q, axis=1), n_eval=100)\n",
    "                success_rates.append(success_rate)\n",
    "\n",
    "        value = np.max(q, axis=1)\n",
    "        pi = np.argmax(q, axis=1)\n",
    "\n",
    "        return value, pi, success_rates\n",
    "\n",
    "\n",
    "# Run QLearning\n",
    "env: FrozenLakeEnv = gym.make(\"FrozenLake-v1\", map_name=\"4x4\")\n",
    "\n",
    "q_learning = QLearning(gamma, alpha, t_decay, n_iters=20000)\n",
    "value, pi, success_rates = q_learning(env)\n",
    "\n",
    "plot_value_function(value, env)\n",
    "save_figure(\"vf_q\")\n",
    "plot_learning_curve(success_rates)\n",
    "save_figure(\"lc_q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 3.2 (3 Points)\n",
    "You may have noticed that we used the 4x4 version of the environment for the Q-Learning algorithm instead of the 8x8 one. You will find that the 8x8 version does not learn anything. Find out why this is the case and propose a solution for it. Give a short and precise summary of what you would do in 3-5 sentences (3 Points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run QLearning on the 8x8 environment\n",
    "\n",
    "env: FrozenLakeEnv = gym.make(\"FrozenLake-v1\", map_name=\"8x8\")\n",
    "\n",
    "value, pi, success_rates = q_learning(env)\n",
    "\n",
    "plot_value_function(value, env)\n",
    "save_figure(\"vf_q_8x8\")\n",
    "plot_learning_curve(success_rates)\n",
    "save_figure(\"lc_q_8x8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
