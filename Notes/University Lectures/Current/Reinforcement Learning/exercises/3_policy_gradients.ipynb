{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Setup\n",
    "\n",
    "If you prefer to work locally, see the following instructions for setting up Python in a virtual environment.\n",
    "You can then ignore the instructions in \"Colab Setup\".\n",
    "\n",
    "If you haven't yet, create a [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html) environment using:\n",
    "```\n",
    "conda create --name rl_exercises\n",
    "conda activate rl_exercises\n",
    "```\n",
    "Torch recommends installation using conda rather than pip, so run:\n",
    "```\n",
    "conda install pytorch cpuonly -c pytorch\n",
    "```\n",
    "If you have a CUDA-enabled GPU and would like to use it, visit [the installation page](https://pytorch.org/get-started/locally/) to see the options available for different CUDA versions.\n",
    "The remaining dependencies can be installed with pip:\n",
    "```\n",
    "pip install matplotlib numpy tqdm \"gymnasium[classic-control, other]\"\n",
    "```\n",
    "\n",
    "Even if you are running the Jupyter notebook locally, please run the code cells in **Colab Setup**, because they define some global variables required later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab Setup\n",
    "\n",
    "Google Colab provides you with a temporary environment for python programming.\n",
    "While this conveniently works on any platform and internally handles dependency issues and such, it also requires you to set up the environment from scratch every time.\n",
    "The \"Colab Setup\" section below will be part of **every** exercise and contains utility that is needed before getting started.\n",
    "\n",
    "There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window).\n",
    "Any changes you make to the Jupyter notebook itself should be saved to your Google Drive.\n",
    "We also save all recordings and logs in it by default so that you won't lose your work in the event of an instance timeout.\n",
    "However, you will need to re-mount your Google Drive and re-install packages with every new instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4HBPnmbIPPyl",
    "outputId": "c9f8b25f-f30a-472f-f2ce-989a18cc64af"
   },
   "outputs": [],
   "source": [
    "\"\"\"Your work will be stored in a folder called `rl_ws23` by default to prevent Colab \n",
    "instance timeouts from deleting your edits.\n",
    "We do this by mounting your google drive on the virtual machine created in this colab \n",
    "session. For this, you will likely need to sign in to your Google account and allow\n",
    "access to your Google Drive files.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    COLAB = True\n",
    "except ImportError:\n",
    "    COLAB = False\n",
    "\n",
    "# Create paths in your google drive\n",
    "if COLAB:\n",
    "    DATA_ROOT = Path(\"/content/gdrive/My Drive/rl_ws23\")\n",
    "    DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    DATA_ROOT_STR = str(DATA_ROOT)\n",
    "    %cd \"$DATA_ROOT\"\n",
    "else:\n",
    "    DATA_ROOT = Path.cwd() / \"rl_ws23\"\n",
    "\n",
    "# Install python packages\n",
    "if COLAB:\n",
    "    %pip install matplotlib numpy tqdm \"gymnasium[classic-control, other]\" torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLXw6zd-k3Xd"
   },
   "source": [
    "# Exercise 3\n",
    "\n",
    "In this homework we will be mainly working on Policy gradients (Lecture 5) \n",
    "and Natural Policy Gradients (Lecture 6). We are going to implement the \n",
    "REINFORCE, the Policy Gradient Theorem and the Natural Gradient algorithms. \n",
    "\n",
    "All homeworks are self-contained.\n",
    "They can be completed in their respective notebooks.\n",
    "To edit and re-run code, you can therefore simply edit and restart the code cells below.\n",
    "When you are finished, you will need to submit the notebook as well as all saved figures (see exercises) as a zip file via Ilias.\n",
    "\n",
    "We start by importing all the necessary python modules and defining some helper functions which you do not need to change.\n",
    "Still, make sure you are aware of what they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "enh5ZMHftEO7"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "# Set random seed and output paths\n",
    "SEED = 3\n",
    "OUTPUT_FOLDER = DATA_ROOT / \"exercise_3\" / time.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "class ProgressBar:\n",
    "    def __init__(self, num_iterations: int, verbose: bool = True):\n",
    "        if verbose:  # create a nice little progress bar\n",
    "            self.scalar_tracker = tqdm.tqdm(\n",
    "                total=num_iterations,\n",
    "                desc=\"Scalars\",\n",
    "                bar_format=\"{desc}\",\n",
    "                position=0,\n",
    "                leave=True,\n",
    "            )\n",
    "            progress_bar_format = (\n",
    "                \"{desc} {n_fmt:\"\n",
    "                + str(len(str(num_iterations)))\n",
    "                + \"}/{total_fmt}|{bar}|{elapsed}<{remaining}\"\n",
    "            )\n",
    "            self.progress_bar = tqdm.tqdm(\n",
    "                total=num_iterations,\n",
    "                desc=\"Iteration\",\n",
    "                bar_format=progress_bar_format,\n",
    "                position=1,\n",
    "                leave=True,\n",
    "            )\n",
    "            self.verbose = True\n",
    "        else:\n",
    "            self.verbose = False\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        if self.verbose:\n",
    "            formatted_scalars = {\n",
    "                key: \"{:.3e}\".format(value[-1] if isinstance(value, list) else value)\n",
    "                for key, value in kwargs.items()\n",
    "            }\n",
    "            description = \"Scalars: \" + \", \".join(\n",
    "                [f\"{key}={value}\" for key, value in formatted_scalars.items()]\n",
    "            )\n",
    "            self.scalar_tracker.set_description(description)\n",
    "            self.progress_bar.update(1)\n",
    "\n",
    "\n",
    "def get_reward_contours(\n",
    "    b_min: float = -10,\n",
    "    b_max: float = 0,\n",
    "    n_points: int = 50,\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Compute reward for policies with all combinations of parameters between b_min and b_max\n",
    "    in steps of n_points. The policy is tested with the mean predicted action and zero variance.\n",
    "    Since this is result is deterministic, each policy is only tested once.\n",
    "    :return:\n",
    "        k1s: np.ndarray [n_points]: values of k1 tested\n",
    "        k2s: np.ndarray [n_points]: values of k2 tested\n",
    "        rewards: np.ndarray [n_points, n_points]: reward for each policy\n",
    "    \"\"\"\n",
    "    env = LinEnv()\n",
    "    policy = LinPolicy()\n",
    "    k1s = np.linspace(start=b_min, stop=b_max, num=n_points)\n",
    "    k2s = np.linspace(start=b_min, stop=b_max, num=n_points)\n",
    "    rewards = np.zeros((n_points, n_points))\n",
    "    for i, k1 in enumerate(k1s):\n",
    "        for j, k2 in enumerate(k2s):\n",
    "            c_params = np.array([k1, k2, 1])\n",
    "            policy.update_params(c_params)\n",
    "            rewards[j, i] = PolicyGradientAlgo.test_policy(\n",
    "                policy=policy,\n",
    "                env=env,\n",
    "                n_trials=1,\n",
    "            )\n",
    "    rewards = np.clip(rewards, -1500, 1500)\n",
    "    return k1s, k2s, rewards\n",
    "\n",
    "\n",
    "def plot_rewards(\n",
    "    *reward_curves: np.ndarray,\n",
    "    colors: Sequence[str],\n",
    "    labels: Sequence[str],\n",
    ") -> None:\n",
    "    plt.figure()\n",
    "    for reward_curve, color, label in zip(reward_curves, colors, labels):\n",
    "        plt.plot(reward_curve, color=color, label=label)\n",
    "    plt.title(\"Policy Performance during Training\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Mean Rewards\")\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "def plot_gradients(\n",
    "    *gradient_curves: np.ndarray,\n",
    "    colors: Sequence[str],\n",
    "    labels: Sequence[str],\n",
    ") -> None:\n",
    "    plt.figure()\n",
    "    for gradient_curve, color, label in zip(gradient_curves, colors, labels):\n",
    "        plt.plot(gradient_curve.mean(axis=-1), color=color, label=label)\n",
    "    plt.title(\"Mean Gradient during Training\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Mean of Parameter Gradients\")\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "def plot_parameter_contour(\n",
    "    *parameter_curves: np.ndarray,\n",
    "    colors: Sequence[str],\n",
    "    labels: Sequence[str],\n",
    ") -> None:\n",
    "    k1s, k2s, rewards = reward_contour_data\n",
    "    fig, ax = plt.subplots()\n",
    "    CS = ax.contour(*np.meshgrid(k1s, k2s), rewards)\n",
    "    ax.clabel(CS, inline=True, fontsize=10)\n",
    "\n",
    "    for parameter_curve, color, label in zip(parameter_curves, colors, labels):\n",
    "        plt.plot(\n",
    "            parameter_curve[:, 0],\n",
    "            parameter_curve[:, 1],\n",
    "            color=color,\n",
    "            alpha=0.5,\n",
    "            label=label,\n",
    "        )\n",
    "        plt.plot(\n",
    "            parameter_curve[:, 0],\n",
    "            parameter_curve[:, 1],\n",
    "            \"x\",\n",
    "            color=color,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        plt.plot(\n",
    "            parameter_curve[-1, 0],\n",
    "            parameter_curve[-1, 1],\n",
    "            \"x\",\n",
    "            color=\"r\",\n",
    "        )\n",
    "    plt.title(\"Trajectory Through Parameter Space\")\n",
    "    plt.xlabel(\"k1\")\n",
    "    plt.ylabel(\"k2\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingResults:\n",
    "    test_rewards: np.ndarray\n",
    "    grad_estimates: np.ndarray\n",
    "    parameters: np.ndarray\n",
    "\n",
    "\n",
    "def plot_training_results(\n",
    "    *results: TrainingResults,\n",
    "    colors: Sequence[str],\n",
    "    labels: Sequence[str],\n",
    "    prefix: str,\n",
    ") -> None:\n",
    "    # plot rewards of policy during training\n",
    "    plot_rewards(\n",
    "        *[result.test_rewards for result in results],\n",
    "        colors=colors,\n",
    "        labels=labels,\n",
    "    )\n",
    "    save_current_figure(prefix + \"_rewards\")\n",
    "    plt.show()\n",
    "\n",
    "    # plot magnitude of gradients\n",
    "    plot_gradients(\n",
    "        *[result.grad_estimates for result in results],\n",
    "        colors=colors,\n",
    "        labels=labels,\n",
    "    )\n",
    "    save_current_figure(prefix + \"_grad_estimates\")\n",
    "    plt.show()\n",
    "\n",
    "    # plot trajectory of parameters through the parameter space\n",
    "    plot_parameter_contour(\n",
    "        *[result.parameters for result in results],\n",
    "        colors=colors,\n",
    "        labels=labels,\n",
    "    )\n",
    "    save_current_figure(prefix + \"_contour\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_current_figure(save_name: str) -> None:\n",
    "    plt.savefig(str(OUTPUT_FOLDER / f\"{save_name}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bj_5XyqHZVYZ"
   },
   "source": [
    "## Continuous Control with a Linear Controller\n",
    "\n",
    "We are going to consider a linear dynamical system with a quadratic reward function.\n",
    "We will use various algorithms to optimize the parameters of a linear controller.\n",
    "The learning agent does not have any information about the system dynamics and the reward function. \n",
    "\n",
    "### Linear System and Quadratic Reward Function\n",
    "\n",
    "The linear dynamics are described as follows:\n",
    "\n",
    "\\begin{align}\n",
    "      \\boldsymbol{s'} = \n",
    "       \\boldsymbol{As} + \\boldsymbol{Ba},\n",
    "\\end{align}\n",
    "where $\\boldsymbol{s'}$ denotes the state in the next time step and $\\boldsymbol{a}$ is the action input to the system. The identites of the system are given as \n",
    "\\begin{align}\n",
    "    \\boldsymbol{A} = \\begin{bmatrix}\n",
    "                        1 & 0.1\\\\\n",
    "                        0 & 0.99\n",
    "                      \\end{bmatrix}, ~~~~~\n",
    "     \\boldsymbol{B} = \\begin{bmatrix}\n",
    "                        0 \\\\\n",
    "                        0.1 \n",
    "                       \\end{bmatrix}.\n",
    "\\end{align}\n",
    "Thus, we have a two dimensional state-space and a one dimensional action space.\n",
    "\n",
    "The immediate reward function, is given as\n",
    "\\begin{align}\n",
    "    r(\\boldsymbol{s_t}, a_t) = -\\boldsymbol{s_t}^T\\boldsymbol{M}\\boldsymbol{s_t} - a_t^2Q,\n",
    "\\end{align}\n",
    "resulting in an episode reward\n",
    "\\begin{align}\n",
    "    R(\\tau) = \\sum_t r(\\boldsymbol{s_t}, a_t)\n",
    "\\end{align}\n",
    "\n",
    "The code block which describes the linear system and its corresponding quadratic reward function \n",
    "is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Dydj9zIzlIH"
   },
   "outputs": [],
   "source": [
    "S_DIM = 2\n",
    "A_DIM = 1\n",
    "HORIZON = 50\n",
    "\n",
    "\n",
    "class LinEnv:\n",
    "    _A = np.array([[1, 0.1], [0, 0.99]])\n",
    "    _B = np.array([0, 0.1])\n",
    "    _M = np.array([[10, 0], [0, 1]])\n",
    "    _Q = np.array([1])\n",
    "    _s_init = np.array([2, 1])\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.t = 0\n",
    "        self._s = np.array([2, 1])\n",
    "        self.s_max = np.ones(2) * 12\n",
    "        self.s_min = -np.ones(2) * 12\n",
    "        self.a_max = 8\n",
    "        self.a_min = -8\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        self._s = self._s_init\n",
    "        self.t = 0\n",
    "        return self._s.copy()\n",
    "\n",
    "    def step(self, action: np.ndarray) -> tuple[np.ndarray, float]:\n",
    "        reward = -self._s.T @ self._M @ self._s - action.T * self._Q * action\n",
    "        clipped_action = np.clip(action, self.a_min, self.a_max)\n",
    "        self._s = self._A @ self._s + self._B * clipped_action\n",
    "        self._s = np.clip(self._s, self.s_min, self.s_max)\n",
    "        self.t += 1\n",
    "        return self._s.copy(), reward.item()\n",
    "\n",
    "\n",
    "linear_env = LinEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Linear Controller**\n",
    "We consider a linear, stochastic controller of the form\n",
    "\\begin{align}\n",
    "    \\pi(a|\\boldsymbol{s}) = \\mathcal{N}(a|\\boldsymbol{Ks}, \\sigma^2) =\\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{1}{2}\\frac{(a-\\boldsymbol{Ks})^2}{\\sigma^2}},\n",
    "\\end{align}\n",
    "where $\\boldsymbol{K} = [k_1, k_2]$ and $\\sigma$ are the learnable parameters.\n",
    "Our learning algorithms will optimize these parameters.\n",
    "\n",
    "The following code defines the linear controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinPolicy:\n",
    "    _params: np.ndarray\n",
    "    _K: np.ndarray\n",
    "    _std: np.ndarray  # standard deviation, i.e. sigma\n",
    "\n",
    "    def __init__(self):\n",
    "        self._K = np.zeros(S_DIM)\n",
    "        self._std = np.ones(A_DIM)\n",
    "        self.update_params(np.array([-10, -10, 1]))\n",
    "\n",
    "    @property\n",
    "    def params(self) -> np.ndarray:\n",
    "        return self._params\n",
    "\n",
    "    @property\n",
    "    def n_params(self) -> int:\n",
    "        return self._params.shape[0]\n",
    "\n",
    "    def update_params(self, params: np.ndarray) -> None:\n",
    "        assert len(params) == S_DIM + A_DIM\n",
    "        self._K[:] = params[:S_DIM]\n",
    "        self._std[:] = params[S_DIM:]\n",
    "        self._params = params\n",
    "\n",
    "    def get_mean(self, state: np.ndarray) -> np.ndarray:\n",
    "        # expand dims so that dimension matches that of action\n",
    "        # dot product removes one dimension\n",
    "        return np.expand_dims(state @ self._K, axis=-1)\n",
    "\n",
    "    def sample(self, state: np.ndarray) -> np.ndarray:\n",
    "        mean = self.get_mean(state)\n",
    "        action = np.random.normal(mean, np.abs(self._std))\n",
    "        return action\n",
    "\n",
    "    def grad_log_pi(self, states: np.ndarray, actions: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Get the gradient of the log probability of the given action(s) in the given state(s).\n",
    "        :param states: np.ndarray [..., S_DIM], where S_DIM is the state dimension\n",
    "        :param actions: np.ndarray [..., A_DIM], where A_DIM is the action dimension\n",
    "        \"\"\"\n",
    "        std_inv = 1 / (self._std + 1e-20)\n",
    "        z = actions - self.get_mean(states)\n",
    "        grad_K = z * states * (std_inv**2)\n",
    "        grad_sigma = -std_inv + (z**2) * (std_inv**3)\n",
    "        return np.concatenate((grad_K, grad_sigma), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Algorithms\n",
    "\n",
    "Next, we will create a simple base class for policy gradient algorithms, which all have the same structure.\n",
    "This structure is generally:\n",
    "\n",
    "---\n",
    "\n",
    "- **Repeat**  For $k=1, 2, \\dots$\n",
    "    - run policy: sample trajectories \\{$\\tau_i$\\}, ${i=1,...,N}$ from $\\pi_{\\boldsymbol{\\theta}}(a|\\boldsymbol{s})$\n",
    "    - Estimate the gradient $\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$ : estimate_grad()\n",
    "    - Update the parameters: grad_ascent_step()\n",
    "\n",
    "- **Until convergence**\n",
    "\n",
    "---\n",
    "\n",
    "Based on this general structure, in the following we define a base class `PolicyGradientAlgorithm`, which contains shared attributes and functions.\n",
    "The function `train` implements the main algorithm loop.\n",
    "In this exercise, we will use various learning rates (`lr`) for various algorithms, with and without baselines.\n",
    "The parameters `n_iters` and `batch_size` stay the same for the algorithms.\n",
    "Also note that for this continuous control problem, the discount factor $\\gamma$ is omitted since we set it to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientAlgo:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: LinEnv,\n",
    "        policy: LinPolicy,\n",
    "        n_iters: int = 150,\n",
    "        batch_size: int = 25,\n",
    "        lr: float = 1e-4,\n",
    "        baseline: bool = False,\n",
    "    ) -> None:\n",
    "        self.env = env  # current environment\n",
    "        self.policy = policy  # current policy object\n",
    "        self.n_iters = n_iters  # number of total iterations\n",
    "        self.batch_size = batch_size  # number of traj. samples per iteration\n",
    "        self.lr = lr  # learning rate\n",
    "        self.baseline = baseline  # whether to use baseline or not\n",
    "\n",
    "    def estimate_grad(\n",
    "        self,\n",
    "        rewards: np.ndarray,\n",
    "        states: np.ndarray,\n",
    "        actions: np.ndarray,\n",
    "    ) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(self) -> TrainingResults:\n",
    "        \"\"\"\n",
    "        This function will perform the main loop of the policy gradient algorithms.\n",
    "        For plotting and saving purposes, it will return the estimated gradients,\n",
    "        the test rewards and the parameters of each iteration.\n",
    "        :return:\n",
    "            TrainingResults:\n",
    "                estimated_grads: np.ndarray [n_iters x n_params]\n",
    "                test_rewards: np.ndarray [n_iters]\n",
    "                parameters: np.ndarray[n_iters+1 x n_params]\n",
    "        \"\"\"\n",
    "        grads = np.empty((self.n_iters, self.policy.n_params))\n",
    "        test_rewards = np.empty(self.n_iters)\n",
    "        rewards = np.empty((self.batch_size, HORIZON))\n",
    "        states = np.empty((self.batch_size, HORIZON, S_DIM))\n",
    "        actions = np.empty((self.batch_size, HORIZON, A_DIM))\n",
    "        parameters = np.empty((self.n_iters + 1, self.policy.n_params))\n",
    "        parameters[0] = self.policy.params\n",
    "        progress_bar = ProgressBar(num_iterations=self.n_iters)\n",
    "        for k in range(self.n_iters):\n",
    "            for j in range(self.batch_size):\n",
    "                state = self.env.reset()\n",
    "                for t in range(HORIZON):\n",
    "                    states[j, t] = state\n",
    "                    actions[j, t] = self.policy.sample(state)\n",
    "                    state, rewards[j, t] = self.env.step(actions[j, t])\n",
    "            grads[k] = self.estimate_grad(rewards, states, actions)\n",
    "            parameters[k + 1] = self.grad_ascent_step(grads[k], states, actions)\n",
    "            self.policy.update_params(parameters[k + 1])\n",
    "            test_rewards[k] = self.test_policy(policy=self.policy, env=self.env)\n",
    "            progress_bar(test_reward=test_rewards[k])\n",
    "        return TrainingResults(test_rewards, grads, parameters)\n",
    "\n",
    "    def grad_ascent_step(\n",
    "        self,\n",
    "        grad_estimates: np.ndarray,\n",
    "        states: np.ndarray | None = None,\n",
    "        actions: np.ndarray | None = None,\n",
    "    ):\n",
    "        \"\"\"This function performs the gradient ascent step.\n",
    "        :param grad_estimates: estimates of the gradients for all parameters:\n",
    "            np.ndarray [n_params]\n",
    "        :param states all states of the last iteration:\n",
    "            np.ndarray [batch_size x HORIZON x S_DIM], where S_DIM is the state dimension\n",
    "        :param actions all taken actions of the last iteration:\n",
    "            np.ndarray [batch_size x HORIZON x A_DIM], where A_DIM is the action dimension\n",
    "        :return: updated policy parameters: np.ndarray [n_params]\n",
    "        \"\"\"\n",
    "        return self.policy.params + self.lr * grad_estimates\n",
    "\n",
    "    @staticmethod\n",
    "    def test_policy(policy: LinPolicy, env: LinEnv, n_trials: int = 10) -> float:\n",
    "        ep_rewards = np.empty((n_trials, HORIZON))\n",
    "        for i in range(n_trials):\n",
    "            s = env.reset()\n",
    "            for t in range(HORIZON):\n",
    "                a = policy.get_mean(s)\n",
    "                s, r = env.step(a)\n",
    "                ep_rewards[i, t] = r\n",
    "        return np.mean(np.sum(ep_rewards, axis=1)).item()\n",
    "\n",
    "\n",
    "# evaluate a grid of policy parameters for plotting later\n",
    "reward_contour_data = get_reward_contours()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **REINFORCE**\n",
    "We start with the most basic algorithm, **REINFORCE**. The **pseudocode** is given as\n",
    "\n",
    "---\n",
    "\n",
    "- **Repeat**  For $k=1, 2, \\dots$\n",
    "    - run policy: sample trajectories \\{$\\tau_i$\\}, ${i=1,...,N}$ from $\\pi_{\\boldsymbol{\\theta}}(a|\\boldsymbol{s})$\n",
    "    - Estimate the gradient:\n",
    "$\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta}) \\approx \\frac{1}{N}\\sum_{i} \\left(\\sum_{t}\\nabla_{\\boldsymbol{\\theta}}\\log \\pi_{\\boldsymbol{\\theta}}(a_{i,t}|\\boldsymbol{s}_{i,t})\\right)\\left(\\sum_{t}\\gamma^tr(\\boldsymbol{s}_{i,t}a_{i,t})\\right)$\n",
    "    - Update the parameters:\n",
    "        - $\\boldsymbol{\\theta}\\leftarrow \\boldsymbol{\\theta} + \\alpha \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$\n",
    "\n",
    "- **Until convergence**\n",
    "\n",
    "---\n",
    "\n",
    "The following class inherits from the `PolicyGradientAlgorithm` class and only needs to override the function 'estimate_grad' according to the pseudo code given above.\n",
    "This function estimates the gradient of the return with respect to the parameters of the policy. \n",
    "\n",
    "## Task 1: REINFORCE (3 Points)\n",
    "Your task is to implement the `estimate_grad` function according to the pseudocode shown above (**Don't implement the policy update!**).\n",
    "Through the object attribute `self.baseline`, we decide if we would like to estimate the gradient with or without a baseline. \n",
    "Make sure that both options are working in your code. \n",
    "Use the following baseline\n",
    "\\begin{align}\n",
    "    b = \\frac{1}{N}\\sum_i\\sum_tr(\\boldsymbol{s}_{i,t}, a_{i,t})\n",
    "\\end{align}\n",
    "\n",
    "*Hint: To get the gradient of the log policy for current state $\\boldsymbol{s}_{i,t}$ and current action $a_{i,t}$ from the rollout i at time step t, you will need to call `self.policy.grad_log_pi(current_state, current_action)`, where current_state is $\\boldsymbol{s}_{i,t}$ and current action is $a_{i,t}$.*\n",
    "\n",
    "After you completed the implementation, run the cell.\n",
    "After training, it plots the reward curve, the magnitude of the gradients, and the trajectory of the parameters through the parameter space.\n",
    "We can now compare the performance of the algorithm with and without a baseline.\n",
    "Notice that with a baseline, we use a learning rate of $10^{-3}$, whereas without a baseline, we use a learning rate of $10^{-4}$.\n",
    "What happens to each version of the algorithm for different learning rates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE(PolicyGradientAlgo):\n",
    "    def estimate_grad(\n",
    "        self,\n",
    "        rewards: np.ndarray,\n",
    "        states: np.ndarray,\n",
    "        actions: np.ndarray,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        This function returns the gradient estimate of the return.\n",
    "        :param rewards: all training rewards of the last iteration:\n",
    "            np.ndarray [batch_size x HORIZON]\n",
    "        :param states: all states of the last iteration:\n",
    "            np.ndarray [batch_size x HORIZON x S_DIM], where S_DIM is the state dimension\n",
    "        :param actions: all taken actions of the last iteration:\n",
    "            np.ndarray [batch_size x HORIZON x A_DIM], where A_DIM is the action dimension\n",
    "        :return: grad_estimate: the estimated gradient: np.ndarray [n_params]\n",
    "        \"\"\"\n",
    "        ## TODO ##\n",
    "        if self.baseline:\n",
    "            advantages = ...\n",
    "        else:\n",
    "            advantages = ...\n",
    "        grad_estimate = ...\n",    "\n",
    "        assert grad_estimate.shape == (3,)\n",
    "        return grad_estimate\n",
    "\n",
    "\n",
    "np.random.seed(SEED)\n",
    "reinforce = REINFORCE(\n",
    "    env=linear_env,\n",
    "    policy=LinPolicy(),\n",
    "    lr=1e-4,\n",
    "    baseline=False,\n",
    ")\n",
    "reinforce_results = reinforce.train()\n",
    "\n",
    "np.random.seed(SEED)\n",
    "reinforce_with_baseline = REINFORCE(\n",
    "    env=linear_env,\n",
    "    policy=LinPolicy(),\n",
    "    lr=1e-3,\n",
    "    baseline=True,\n",
    ")\n",
    "reinforce_with_baseline_results = reinforce_with_baseline.train()\n",
    "\n",
    "plot_training_results(\n",
    "    reinforce_results,\n",
    "    reinforce_with_baseline_results,\n",
    "    colors=[\"cornflowerblue\", \"blue\"],\n",
    "    labels=[\"REINFORCE\", \"REINFORCE with baseline\"],\n",
    "    prefix=\"reinforce\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Theorem\n",
    "\n",
    "**REINFORCE** suffers from high variance in the gradient estimates.\n",
    "One way to reduce this high variance is to exploit the temporal structure of the trajectory when estimating returns.\n",
    "This means using the Monte-Carlo estimate of the return to go from the current state, instead of the total return for the whole trajectory.\n",
    "The pseudo code is given as\n",
    "\n",
    "---\n",
    "\n",
    "- **Repeat**  For $k=1, 2, \\dots$\n",
    "    - run policy: sample trajectories \\{$\\tau_i$\\}, ${i=1,...,N}$ from $\\pi_{\\boldsymbol{\\theta}}(a|\\boldsymbol{s})$\n",
    "    - Estimate the gradient:\n",
    "$\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta}) \\approx \\frac{1}{N}\\sum_{i} \\sum_{t}\\nabla_{\\boldsymbol{\\theta}}\\log \\pi_{\\boldsymbol{\\theta}}(a_{i,t}|\\boldsymbol{s}_{i,t})\\left(\\sum_{k=t}\\gamma^{k-t}r(\\boldsymbol{s}_{i,k}a_{i,k}\\right)$\n",
    "    - Update the parameters:\n",
    "        - $\\boldsymbol{\\theta}\\leftarrow \\boldsymbol{\\theta} + \\alpha \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$\n",
    "\n",
    "- **Until convergence**\n",
    "\n",
    "---\n",
    "\n",
    "The following class inherits from the `PolicyGradientAlgorithm` class and only needs to override the function 'estimate_grad' according to the pseudo code given above.\n",
    "This function estimates the gradient of the return with respect to the parameters of the policy.\n",
    "\n",
    "## Task 2: Policy Gradient Theorem (3 Points)\n",
    "Your task is to implement the `estimate_grad` function according to the pseudocode shown above (**Don't implement the policy update!**).\n",
    "Through the object attribute `self.baseline`, we decide if we would like to estimate the gradient with or without a baseline. \n",
    "Make sure that both options are working in your code. \n",
    "\n",
    "Use the following **time-dependent** baseline for the return starting from state $s_t$\n",
    "\\begin{align}\n",
    "    b_t = \\frac{1}{N}\\sum_i\\sum_{k=t}r(\\boldsymbol{s}_{i,k}, a_{i,k})\n",
    "\\end{align}\n",
    "\n",
    "After you completed the implementation, run the **Execute Policy Gradient Theorem** cell. This cell will save three different figures which you will need to submit.\n",
    "\n",
    "*Hint: To get the gradient of the log policy for current state $\\boldsymbol{s}_{i,t}$ and current action $a_{i,t}$ from the rollout i at time step t, you will need to call `self.policy.grad_log_pi(current_state, current_action)`, where current_state is $\\boldsymbol{s}_{i,t}$ and current action is $a_{i,t}$.*\n",
    "\n",
    "After you completed the implementation, run the cell.\n",
    "How do the performance and stability compare to the REINFORCE algorithm?\n",
    "What happens to each version of the algorithm for different learning rates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientTheorem(PolicyGradientAlgo):\n",
    "    def estimate_grad(\n",
    "        self,\n",
    "        rewards: np.ndarray,\n",
    "        states: np.ndarray,\n",
    "        actions: np.ndarray,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        This function returns the gradient estimate of the return.\n",
    "        :param rewards: all training rewards of the last iteration: np.ndarray\n",
    "                     [batch_size x HORIZON]\n",
    "        :param states: all states of the last iteration: np.ndarray\n",
    "                     [batch_size x HORIZON x S_DIM], where S_DIM is the state dimension\n",
    "        :param actions: all taken actions of the last iteration: np.ndarray\n",
    "                     [batch_size x HORIZON x A_DIM], A_DIM is the action dimension\n",
    "        :return: grad_estimate: the estimated gradient: np.ndarray [n_params]\n",
    "        \"\"\"\n",
    "        ## TODO ##\n",
    "        if self.baseline:\n",
    "            ...\n",
    "        grad_estimate = ...\n",    "\n",
    "        assert grad_estimate.shape == (3,)\n",
    "        return grad_estimate\n",
    "\n",
    "\n",
    "np.random.seed(SEED)\n",
    "pgt = PolicyGradientTheorem(\n",
    "    env=linear_env,\n",
    "    policy=LinPolicy(),\n",
    "    lr=1e-4,\n",
    "    baseline=False,\n",
    ")\n",
    "pgt_results = pgt.train()\n",
    "\n",
    "np.random.seed(SEED)\n",
    "pgt_with_baseline = PolicyGradientTheorem(\n",
    "    env=linear_env,\n",
    "    policy=LinPolicy(),\n",
    "    lr=1e-3,\n",
    "    baseline=True,\n",
    ")\n",
    "pgt_with_baseline_results = pgt_with_baseline.train()\n",
    "\n",
    "plot_training_results(\n",
    "    pgt_results,\n",
    "    pgt_with_baseline_results,\n",
    "    colors=[\"olive\", \"green\"],\n",
    "    labels=[\"Policy Gradient Theorem\", \"Policy Gradient Theorem with baseline\"],\n",
    "    prefix=\"pg_theorem\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Natural Policy Gradient**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common approach in policy search is to apply a trust region constraint, where the policy update is bounded.\n",
    "It has been shown that trust regions highly stabilize the learning process. \n",
    "\n",
    "A trust region approach which can be applied to continous control problems with parametric policy distributions is **Natural Policy Gradient**.\n",
    "Here, the KL-constraint is approximated with the second order Taylor approximation, which results in the **Fisher Information** matrix $F$.\n",
    "\n",
    "Concretely, we can calculate $F$ as \n",
    "\\begin{align}\n",
    "    \\boldsymbol{F} = \\frac{1}{TN}\\sum_i^N\\sum_t^T \\left[\\nabla_{\\theta}\\log \\pi_{\\theta}(a_{i,t}|s_{i,t})\\nabla_{\\theta}\\log \\pi_{\\theta}(a_{i,t}|s_{i,t})^T\\right].\n",
    "\\end{align}\n",
    "\n",
    "The policy's parameter update is then given as\n",
    "\\begin{align}\n",
    "    \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\alpha\\boldsymbol{F}^{-1}\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta}),\n",
    "\\end{align}\n",
    "where for the gradient estimate $\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$ standard techniques like the policy gradient theorem is used.\n",
    "\n",
    "In Natural Policy Gradient, the learning rate parameter $\\alpha ~~(\\eta^{-1}\\text{ in the slides})$ can be solved in closed-form by finding the optimal solution of the dual function to the according Lagrangian (see Task 4). More specifically, $\\alpha$ is given as \n",
    "\\begin{align}\n",
    "    \\alpha = \\sqrt{\\frac{4\\epsilon}{\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})^{T}\\boldsymbol{F}^{-1}\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})}},\n",
    "\\end{align}\n",
    "where $\\epsilon$ is the hyperparameter, bounding the expected KL between the new and the old policy.\n",
    "\n",
    "Since we will use the gradient estimate from the **Policy Gradient Theorem**, the following class inherits from the **PolicyGradientTheorem** class, i.e. you need to have properly solved Task 2 in order to be able to solve this task.\n",
    "\n",
    "\n",
    "## Task 3: Natural Policy Gradient (3 Points)\n",
    "\n",
    "Implement the function `get_fisher_information`, which will return the Fisher information matrix given state-action trajectories.\n",
    "Apply the equations mentioned above.\n",
    "Implement the function `grad_ascent_step`, which calculates and returns the new parameters of the policy according to the update rule mentioned above.\n",
    "When implementing the update, please also use the closed-form solution to $\\alpha$ to scale the update.\n",
    "\n",
    "*Note: You will need to have properly solved Task 2 in order to be able to solve this task.*\n",
    "\n",
    "*Hint: To get the gradient of the log policy for current state $\\boldsymbol{s}_{i,t}$ and current action $a_{i,t}$ from the rollout i at time step t, you will need to call `self.policy.grad_log_pi(current_state, current_action)`, where current_state is $\\boldsymbol{s}_{i,t}$ and current action is $a_{i,t}$.*\n",
    "\n",
    "After you completed the implementation, run the cell.\n",
    "What do you notice?\n",
    "How does this algorithm respond to different hyperparameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaturalPolicyGradient(PolicyGradientTheorem):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        eps: float = 0.5,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.eps = eps\n",
    "\n",
    "    def get_fisher_information(\n",
    "        self,\n",
    "        states: np.ndarray,\n",
    "        actions: np.ndarray,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        This function calculates the Fisher Information matrix.\n",
    "        :param states: all states of the last iteration: np.ndarray\n",
    "                     [batch_size x HORIZON x S_DIM], where S_DIM is the state dimension\n",
    "        :param actions: all taken actions of the last iteration: np.ndarray\n",
    "                     [batch_size x HORIZON x A_DIM], A_DIM is the action dimension\n",
    "        :return: F: returns the Fisher information matrix: np.ndarray [n_params x n_params]\n",
    "        \"\"\"\n",
    "        ## TODO ##\n",
    "        F = ...\n",    "\n",
    "        assert F.shape == (3, 3)\n",
    "        return F\n",
    "\n",
    "    def grad_ascent_step(\n",
    "        self,\n",
    "        grad_estimates: np.ndarray,\n",
    "        states: np.ndarray,\n",
    "        actions: np.ndarray,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Performs an updated on the parameters of the policy according to the Natural Policy Gradient Rule by using\n",
    "        the current gradient estimate (grad_estimate) of the Policy Gradient theorem with baseline, the\n",
    "        state trajectories and the action trajectories.\n",
    "        :param grad_estimates: estimates of the gradients for all parameters:\n",
    "            np.ndarray [n_params]\n",
    "        :param states all states of the last iteration:\n",
    "            np.ndarray [batch_size x HORIZON x S_DIM], where S_DIM is the state dimension\n",
    "        :param actions all taken actions of the last iteration:\n",
    "            np.ndarray [batch_size x HORIZON x A_DIM], where A_DIM is the action dimension\n",
    "        :return: updated policy parameters: np.ndarray [n_params]\n",
    "        \"\"\"\n",
    "        ## TODO ##\n",
    "        new_policy_params = ...\n",    "\n",
    "        assert new_policy_params.shape == (3,)\n",
    "        return new_policy_params\n",
    "\n",
    "\n",
    "np.random.seed(SEED)\n",
    "npg = NaturalPolicyGradient(\n",
    "    env=linear_env,\n",
    "    policy=LinPolicy(),\n",
    "    eps=0.5,\n",
    "    baseline=True,\n",
    ")\n",
    "npg_results = npg.train()\n",
    "plot_training_results(\n",
    "    npg_results,\n",
    "    colors=[\"orange\"],\n",
    "    labels=[\"Natural Policy Gradient\"],\n",
    "    prefix=\"npg\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will plot the results from all algorithms into one plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all algorithms in one plot\n",
    "plot_training_results(\n",
    "    reinforce_results,\n",
    "    reinforce_with_baseline_results,\n",
    "    pgt_results,\n",
    "    pgt_with_baseline_results,\n",
    "    npg_results,\n",
    "    colors=[\"cornflowerblue\", \"blue\", \"olive\", \"green\", \"orange\"],\n",
    "    labels=[\n",
    "        \"REINFORCE\",\n",
    "        \"REINFORCE with baseline\",\n",
    "        \"Policy Gradient Theorem\",\n",
    "        \"Policy Gradient Theorem with baseline\",\n",
    "        \"Natural Policy Gradient\",\n",
    "    ],\n",
    "    prefix=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 4: Natural Policy Gradient Step Size** (4 Points)\n",
    "\n",
    "Recall that Natural Gradients use a Taylor approximation of the trust region problem, i.e., the objective is given as \n",
    "$$\\boldsymbol{g}^* = \\underset{\\boldsymbol{g}}{\\textrm{argmax}} ~~ \\boldsymbol{g}^T\\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{J} ~~ \\textrm{s.t.} ~~ \\boldsymbol{g}^T \\boldsymbol{F} \\boldsymbol{g} \\leq \\epsilon.$$\n",
    "By introducing a Lagrangian multiplier $\\eta$ we can construct the corresponding Lagrangian\n",
    "$$ L(\\boldsymbol{g}, \\eta) = \\boldsymbol{g}^T \\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{J} + \\eta \\left(\\epsilon - \\boldsymbol{g}^T\\boldsymbol{F}\\boldsymbol{g}\\right).$$\n",
    "\n",
    ".## TODO ##\n",
    "\n",
    "Derive $\\boldsymbol{g}^*$. Also solve the dual, your solution should not depend on $\\eta$ anymore!\n",
    "\n",
    "You may submit your answer in LaTeX as part of this document, or as a separate document included in the same zip file as your submission.\n",
    "This separate document may be a pdf (e.g. created using word/LaTeX), or a scan of your solution written neatly and legible on a sheet of paper.\n",
    "\n",    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Discrete Control with Deep Neural Net Controller**\n",
    "\n",
    "Next, we will consider training Deep Neural Net policies to solve the discrete pole balancing environment called 'CartPole'. We will use the ['CartPole-v1' environment from Gymnasium](https://gymnasium.farama.org/environments/classic_control/cart_pole/).\n",
    "\n",
    "![The CartPole Environment](https://gymnasium.farama.org/_images/cart_pole.gif)\n",
    "\n",
    "**Policy gradients With Neural Network Policies.**\n",
    "\n",
    "We will use the **REINFORCE** algorithm **with baselines** and **policy gradient theorem** as shown below. \n",
    "\n",
    "---\n",
    "\n",
    "- **Repeat**  For $k=1, 2, \\dots$\n",
    "    - run policy: sample trajectories \\{$\\tau_i$\\}, ${i=1,...,N}$ from $\\pi_{\\boldsymbol{\\theta}}(a|\\boldsymbol{s})$\n",
    "    - Estimate the gradient:\n",
    "$\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta}) \\approx \\frac{1}{N}\\sum_{i} \\sum_{t}\\nabla_{\\boldsymbol{\\theta}}\\log \\pi_{\\boldsymbol{\\theta}}(a_{i,t}|\\boldsymbol{s}_{i,t})\\left(Q_t-b_t\\right)$\n",
    "    - Update the parameters:\n",
    "        - $\\boldsymbol{\\theta}\\leftarrow \\boldsymbol{\\theta} + \\alpha \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$\n",
    "\n",
    "- **Until convergence**\n",
    "\n",
    "---\n",
    "$Q_t$ is the Q-value at time t, $Q^{\\pi}(s_t, a_t)$, and $b_t$ is a baseline.\n",
    "\n",
    "However we will now replace the linear policies with deep neural networks and compute policy gradients with automatic differentiation. To do this we create a graph in such a way that its gradient is the policy gradient. \n",
    "\n",
    "We first import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.distributions as distributions\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def train_once(\n",
    "    env: gym.Env,\n",
    "    policy: nn.Module,\n",
    "    discount: float,\n",
    "    baseline: bool,\n",
    "    pg_theorem: bool,\n",
    ") -> tuple[float, float]:\n",
    "    policy.train()\n",
    "\n",
    "    log_prob_actions: list[torch.Tensor] = []\n",
    "    rewards: list[float] = []\n",
    "\n",
    "    state, info = env.reset()\n",
    "    terminated = truncated = False\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "        state = torch.from_numpy(state).to(device).unsqueeze(dim=0)\n",
    "\n",
    "        # get logits from neural network (unnormalized probabilities)\n",
    "        action_logits = policy(state)\n",
    "\n",
    "        # create a probability distribution using softmax on the logits\n",
    "        action_prob = F.softmax(action_logits, dim=-1)\n",
    "\n",
    "        # Sample discrete actions from a categorical distribution\n",
    "        # https://pytorch.org/docs/stable/distributions.html\n",
    "        dist = distributions.Categorical(action_prob)\n",
    "\n",
    "        # sample an action (without gradients)\n",
    "        action = dist.sample()\n",
    "\n",
    "        # Use distribution object to compute log probabilities for the sampled actions.\n",
    "        # These values have gradients and are used for optimization\n",
    "        log_prob_action = dist.log_prob(action)\n",
    "\n",
    "        state, reward, terminated, truncated, info = env.step(\n",
    "            action.squeeze(dim=0).cpu().numpy()\n",
    "        )\n",
    "\n",
    "        log_prob_actions.append(log_prob_action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    returns = calculate_returns(rewards, discount, baseline, pg_theorem)\n",
    "\n",
    "    loss = update_policy(\n",
    "        returns=torch.from_numpy(returns).to(device),\n",
    "        log_prob_actions=torch.cat(log_prob_actions),\n",
    "    )\n",
    "\n",
    "    return loss, returns[0].item()\n",
    "\n",
    "\n",
    "def evaluate(envs: gym.Env, policy: nn.Module) -> float:\n",
    "    policy.eval()\n",
    "\n",
    "    return_ = 0\n",
    "\n",
    "    state, info = envs.reset()\n",
    "    terminated = truncated = False\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "        state = torch.from_numpy(state).to(device).unsqueeze(dim=0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # get logits from neural network (unnormalized probabilities)\n",
    "            action_logits = policy(state)\n",
    "\n",
    "        # create a probability distribution using softmax on the logits\n",
    "        action_prob = F.softmax(action_logits, dim=-1)\n",
    "\n",
    "        # select the action with the highest probability\n",
    "        action = torch.argmax(action_prob, dim=-1)\n",
    "\n",
    "        state, reward, terminated, truncated, info = envs.step(\n",
    "            action.squeeze(dim=0).cpu().numpy()\n",
    "        )\n",
    "\n",
    "        return_ += reward\n",
    "\n",
    "    return return_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Building a Neural Network in PyTorch (1 point)\n",
    "    \n",
    "This part of the code creates a feed forward neural network using PyTorch, which will form the policy  $\\pi_{\\boldsymbol{\\theta}}(a|\\boldsymbol{s})$.\n",
    "\n",
    "Implement the `__init__` method of the `MLP` class, which creates the neural network as a sequence of alternating `torch.nn.Linear` and `torch.nn.ReLU` layers.\n",
    "After the `__init__` method, `MLP` should have a `network` attribute which is a `torch.nn.Sequential` object.\n",
    "The neural network should have `input_size` input nodes, then hidden layers with sizes dictated by `hidden_sizes`, and finally `output_size` output nodes.\n",
    "\n",
    "*Hint: Take a look at the solutions to Exercise 2 for an example.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        hidden_sizes: list[int],\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        :param state_dim: dimension of the state space\n",
    "        :param state_dim: dimension of the actions\n",
    "        :param hidden_units: list of integers corresponding to hidden units\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        ## TODO ##\n",
    "        self.network = ...\n",    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"forward pass of decoder\n",
    "        :param input:\n",
    "        :return: output mean\n",
    "        \"\"\"\n",
    "        return self.network(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Computing Q-values**\n",
    "The code block computes numpy arrays for Q-values which will be used to compute advantages.\n",
    "\n",
    "Recall that the expression for the policy gradient \"loss\" is\n",
    "\n",
    "\\begin{align}\n",
    "    J = \\text{E}_{\\tau\\sim p(\\tau)}\\left[\\sum_{t=0}^T\\nabla_{\\boldsymbol{\\theta}}\\log \\pi(a_t|s_t)(Q_t-b_t)\\right],\n",
    "\\end{align}\n",
    "where $ \\tau=(s_0, a_0, ...)$ is a trajectory, $Q_t$ is the Q-value at time $t$ and $b_t$ is a baseline.\n",
    "\n",
    "\n",
    "We can obtain four different cases, depending on whether we use returns to returns to go and whether we subtract a baseline or not:\n",
    "\n",
    "**Case 1: REINFORCE (pg_theorem = False)**\n",
    "\n",
    "Here we estimate $Q_t$ by the total discounted return for the entire trajectory, regardless of which time step the Q-value should be for. Therefor, the Q estimator is\n",
    "\n",
    "\\begin{align}\n",
    "    Q_t = \\text{R}(\\tau) = \\sum_{k=0}^T \\gamma^{k} r_{k},\n",
    "\\end{align}\n",
    "\n",
    "**Case 2: Policy Gradient Theorem (pg_theorem = True)**\n",
    "\n",
    "Here, we subtract out any rewards from the past, since they cannot have resulted from the current action. Thus, the Q estimator is\n",
    "\n",
    "\\begin{align}\n",
    "    Q_t = \\text{R}(\\tau_{k \\gt t}) = \\sum_{k=t}^T \\gamma^{(k-t)} r_{k}\n",
    "\\end{align}\n",
    "\n",
    "**Case 3: No Baseline (baseline = False)**\n",
    "\n",
    "Here we use the absolute $Q_t$ as is, with the baseline set to 0:\n",
    "\n",
    "\\begin{align}\n",
    "    b_t = 0\n",
    "\\end{align}\n",
    "\n",
    "**Case 4: With baseline (baseline = True)**\n",
    "\n",
    "We can subtract any baseline from the $Q_t$ as long as it doesn't depend on the trajectory. In this example, since we only have one environment, we take the mean of the $1000$ most recent $Q$ values.\n",
    "\n",
    "\\begin{align}\n",
    "    b_t = \\frac{1}{N}\\sum_i^{N} Q_t\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(\n",
    "    rewards: list[float],\n",
    "    discount: float,\n",
    "    baseline: bool,\n",
    "    pg_theorem: bool,\n",
    ") -> np.ndarray:\n",
    "    returns = []\n",
    "    return_ = 0\n",
    "    for reward in reversed(rewards):\n",
    "        return_ = reward + return_ * discount\n",
    "        returns.append(return_)\n",
    "    returns.reverse()\n",
    "\n",
    "    if not pg_theorem:\n",
    "        returns = returns[:1] * len(returns)\n",
    "\n",
    "    returns = np.array(returns)\n",
    "\n",
    "    if baseline:\n",
    "        recent_returns.extend(returns)\n",
    "        mean_return = np.mean(recent_returns)\n",
    "        returns = returns - mean_return\n",
    "\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Computing Loss (1 Points)\n",
    "\n",
    "Here we create a \"pseudoloss\" which is the weighted maximum likelihood, $\\sum_{t=0}^T \\log \\pi_{\\theta}(a_t|s_t)  (Q_t - b_t )$, using the stored `log_prob_actions` and `returns` values computed in the previous section.\n",
    "The gradient of this loss function with respect to the neural network parameters ($\\theta$) is the policy gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(\n",
    "    returns: torch.Tensor,\n",
    "    log_prob_actions: torch.Tensor,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the loss and backpropagate the errors using pytorch optmizers.\n",
    "    :param returns: rewards received from the previous trajectory\n",
    "        torch.Tensor [HORIZON]\n",
    "    :param log_prob_actions: log probability of each action from the previous trajectory\n",
    "        torch.Tensor [HORIZON]\n",
    "    :param optimizer: the torch optimizer object https://pytorch.org/docs/stable/optim.html\n",
    "    :return: magnitude of calculated loss (float)\n",
    "    \"\"\"\n",
    "\n",
    "    ## TODO ##\n",
    "    loss = ...\n",    "\n",
    "    ## Backpropagate using pytorch autograd. We will use the Adam Optimizer to do this, though any optimizer\n",
    "    ## would work in practice.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters, run training, and plot reward curve\n",
    "\n",
    "We run the algorithm 5 times with different random seeds, and plot the mean reward curve with min/max bounds.\n",
    "\n",
    "You can play around with the hyperparameters and see how each of these 4 affect the algorithms performance. However, please submit the saved figures with the default parameters given here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Hyperparameters\n",
    "# learning\n",
    "baseline = True\n",
    "pg_theorem = True\n",
    "learning_rate = 5e-3\n",
    "discount = 0.99\n",
    "\n",
    "# network architecture\n",
    "input_dim = rl_env.observation_space.shape[0]\n",
    "output_dim = rl_env.action_space.n\n",
    "hidden_sizes = [32, 16]\n",
    "\n",
    "# training length and accelerator\n",
    "N_RUNS = 5\n",
    "N_ITERATIONS = 200\n",
    "device = torch.device(\"cpu\")\n",
    "train_returns = np.zeros((N_RUNS, N_ITERATIONS))\n",
    "test_returns = np.zeros((N_RUNS, N_ITERATIONS))\n",
    "\n",
    "# seeding\n",
    "rl_env.reset(seed=SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "for run in range(N_RUNS):\n",
    "    policy = MLP(input_dim, output_dim, hidden_sizes).to(device)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "    recent_returns = deque(maxlen=1000)\n",
    "\n",
    "    progress_bar = ProgressBar(num_iterations=N_ITERATIONS)\n",
    "    for episode in range(N_ITERATIONS):\n",
    "        loss, train_reward = train_once(rl_env, policy, discount, baseline, pg_theorem)\n",
    "\n",
    "        test_reward = evaluate(rl_env, policy)\n",
    "\n",
    "        train_returns[run][episode] = train_reward\n",
    "        test_returns[run][episode] = test_reward\n",
    "\n",
    "        progress_bar(Run=run, train_reward=train_reward, test_reward=test_reward)\n",
    "\n",
    "\n",
    "# plot the learning curve\n",
    "plt.figure()\n",
    "plt.plot(test_returns.mean(axis=0))\n",
    "plt.fill_between(\n",
    "    np.arange(N_ITERATIONS),\n",
    "    test_returns.min(axis=0),\n",
    "    test_returns.max(axis=0),\n",
    "    alpha=0.1,\n",
    ")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Mean Eval Reward (Min/Max)\")\n",
    "plt.title(\"Mean, Min, and Max Eval Reward During Training\")\n",
    "save_current_figure(\"Deep_PG_Reward\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "eQx7oDGeeKWj"
   ],
   "name": "2_dqn_atari.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
