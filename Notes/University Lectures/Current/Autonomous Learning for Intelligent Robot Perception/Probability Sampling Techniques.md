- Sampling methods are a powerful tool in computer science for approximating deterministic algorithms. They help represent uncertainty without needing a [[Probability Distribution Representation|parametric model]] and can achieve high computational efficiency with minimal approximation error.

## Direct Sampling
- **Objective**: Given a continuous probability distribution $p(x)$, the goal is to generate samples directly from $p$.
- **Intuition**: More samples should be generated in regions where $p(x)$ has high values, as these regions represent higher probabilities.
- **Mathematical Reminder**:
  - The cumulative distribution function (CDF) $P(z)$ represents the probability that a random variable $X$ takes a value less than or equal to $z$:
    $$
    P(z) = \Pr(X \leq z) = \int_{-\infty}^{z} p(x) \, dx
    $$
  - This integral accumulates probabilities from $-\infty$ up to $z$, giving the total probability up to that point.
### Using the CDF for Sampling
![[Pasted image 20241111073651.png#invert|200]]
- **CDF $P(z)$**: The blue curve in the plot represents the CDF, while the red curve represents the probability density function $p(x)$.
- **Direct Sampling Mechanism**:
  - To generate samples according to $p(x)$, we can use the inverse of the CDF, $P^{-1}(u)$, where $u$ is a uniformly distributed random variable in the interval [0,1].
  - The intuition is that $P(z)$ smoothly maps values of $x$ to probabilities, so by inverting this mapping, we can generate values of $x$ that correspond to the probability structure of $p(x)$.
### Inverse Transform Sampling Method
![[Pasted image 20241111073747.png#invert|200]]
- **Sampling Steps**:
  1. Generate a random variable $u$ from a uniform distribution $U(0, 1)$, which provides a random probability level.
  2. Use the inverse of the CDF, $P^{-1}(u)$, to transform this uniformly distributed probability $u$ into a sample $x$ from $p(x)$.
  - **Challenge**: This approach requires the inversion of the CDF $P(z)$, which can be difficult or impossible to obtain analytically for many distributions.
### Drawbacks of Direct Sampling
- **Requirements for Direct Sampling**:
  - A closed form of the CDF $P(z)$: The CDF must be known and expressible in a form that allows integration.
  - The ability to invert $P(z)$: Inverting the CDF is necessary to map uniform random samples $u$ to values $x$ that follow $p(x)$.
- **Practical Limitation**: For many distributions, a closed-form CDF or its inverse is not readily available, making direct sampling challenging or infeasible.
These slides explain the concept of **Normalizing Flow**, which is a method in machine learning for transforming complex probability distributions into simpler ones through a series of invertible transformations. Here’s a breakdown of the main points on each slide:
### Generalizing the Idea: Normalizing Flow
#### Assumptions
   - **Data Space** $\mathcal{X}$: This is the space where the data (such as images, text, etc.) is represented.
   - **Unknown Cumulative Distribution Function (CDF)** $P_X(x) = \Pr(X \leq x)$: This is the probability that a variable $X$ is less than or equal to $x$. However, this CDF is typically unknown.
   - **Invertible Function $f: \mathcal{X} \rightarrow \mathcal{Z}$**: We assume we have an invertible transformation function $f$ that maps data from the data space $\mathcal{X}$ to a simpler space $\mathcal{Z}$, often called the "latent space."
![[Pasted image 20241111074138.png#invert|400]]
#### Latent Distribution
   - The distribution in the latent space, denoted $p(z)$, is related to the distribution in the data space by a **change of variables**. For a single-dimensional case, we have:
     $$
     p(z) = p(f^{-1}(z)) \cdot \left| \frac{d}{dz} f^{-1}(z) \right|
     $$
   - This equation helps us compute the probability distribution in the latent space by transforming it from the data space using the function $f$ and its inverse.
   - **Generalizing for Multi-Dimensional Case**:
     - In higher dimensions, the formula involves the determinant of the Jacobian $J_{f^{-1}}(z)$ of $f^{-1}$:
       $$
       p(x) = p(z) \cdot \left| \det(J_{f^{-1}}(z)) \right|^{-1}
       $$
     - This accounts for how volumes scale under transformation, ensuring that the probability is preserved under the transformation.
#### Generating New Samples
   - With this framework, new samples in the data space can be generated by first sampling from the simpler latent space $\mathcal{Z}$ and then mapping these samples back to the data space $\mathcal{X}$ using the inverse transformation $f^{-1}$.
#### Pros and Cons
- **Advantages**:
  - **Efficient Sampling**: Normalizing flows provide a fast and effective way to generate samples from complex distributions.
  - **Out-of-Distribution Detection**: They can help identify data that doesn’t fit within the distribution used for training, which is useful for anomaly detection.
- **Challenges**:
  - **Dimensional Constraints**: The function $f$ and its inverse must preserve dimensions, meaning the latent space and data space must have the same dimensionality.
  - **Difficulty with Uni-Modal Base Distributions**: If the base distribution in the latent space (like a Gaussian) is uni-modal, representing highly complex or multi-modal data distributions in the data space can be challenging.
## Rejections Sampling
These slides introduce two sampling methods used in probabilistic modeling: **Rejection Sampling** and **Importance Sampling**. Here’s a detailed breakdown of each method:

---

### Rejection Sampling
### Simplified Rejection Sampling Process
![[Pasted image 20241111074621.png#invert|300]]
1. **Objective**: The goal is to generate samples from a target distribution $p(z)$ by using a simpler method of sampling.
2. **Steps**:
   - Assume $p(z) < 1$ for all values of $z$.
   - Sample a point $z$ uniformly over the domain.
   - Sample a value $c$ from the interval $[0, 1]$.
   - **Acceptance Condition**: If $f(z) > c$, accept the sample $z$. Otherwise, reject it.
   - The right plot illustrates this process, where samples above the blue curve $p(z)$ are rejected.
3. **Result**: This method ensures that the samples are more likely to be drawn from high-probability regions of $p(z)$, but some samples are rejected.
### General Case of Rejection Sampling
![[Pasted image 20241111074645.png#invert|400]]
1. **More General Setup**:
   - **Unnormalized Distribution**: Sometimes, we only know $\tilde{p}(z)$, an unnormalized version of $p(z)$. In this case, we can represent $p(z)$ as:
     $$
     p(z) = \frac{1}{Z} \tilde{p}(z)
     $$
     where $Z$ is a normalizing constant.
2. **Proposal Distribution**:
   - Choose a proposal distribution $q(z)$ that is easy to sample from and satisfies $k \cdot q(z) \geq \tilde{p}(z)$ for some constant $k$.
   - **Steps**:
     - Sample $z$ from $q(z)$.
     - Sample a value uniformly from $[0, kq(z)]$.
     - **Rejection Criterion**: Reject the sample if it lies outside $\tilde{p}(z)$.
3. **Efficiency**:
   - While straightforward, rejection sampling can be inefficient, as many samples may be discarded, especially when $q(z)$ is not a close approximation of $p(z)$.
## Importance Sampling
### Concept
![[Pasted image 20241111074729.png#invert|300]]
   - In Importance Sampling, we assign an **importance weight** $w$ to each sample to account for differences between the target distribution $p(x)$ and a proposal distribution $q(x)$.
   - The weight $w(x)$ is calculated as:
     $$
     w(x) = \frac{p(x)}{q(x)}
     $$
   - Here:
     - $p(x)$ is the target distribution.
     - $q(x)$ is the proposal distribution (a simpler distribution from which we can sample easily).
2. **Application**:
   - By adjusting each sample with its weight, we can approximate expectations under $p(x)$ even when we sample from $q(x)$.
### Indicator Function
![[Pasted image 20241111074830.png#invert|200]]
1. **Expectation with Indicator Function**:
   - The probability of a sample falling within an interval $A$ under $p(x)$ can be thought of as the area under $p(x)$ over $A$.
   - This can be represented as an expectation:
     $$
     E_p[I(z \in A)] = \int p(z) I(z \in A) \, dz
     $$
   - Here, $I(z \in A)$ is an indicator function that is 1 if $z \in A$ and 0 otherwise.
### Detailed Formula
1. **Importance Sampling Approximation**:
   - The expectation can be rewritten in terms of the proposal distribution $q(z)$ and the weight $w(z) = \frac{p(z)}{q(z)}$:
     $$
     E_p[I(z \in A)] = \int \frac{p(z)}{q(z)} q(z) I(z \in A) \, dz = E_q[w(z) I(z \in A)]
     $$
2. **Sample Approximation**:
   - In practice, we approximate this expectation by averaging over samples drawn from $q(z)$:
     $$
     E_q[w(z) I(z \in A)] \approx \frac{1}{L} \sum_{l=1}^{L} w(z_l) I(z_l \in A)
     $$
   - This approximation allows us to compute expectations with fewer samples, even when $p(z)$ is complex.
### The Particle Filter
#### Non-Parametric Bayesian Filtering
   - The particle filter is a non-parametric implementation of the Bayesian filter, which is used to estimate the state of a system (like a robot’s position).
   - It represents the **belief** (posterior distribution) over states $Bel(x_t)$ by a set of weighted random samples (particles).

   $$
   Bel(x_t) = \eta p(z_t \mid x_t) \int p(x_t \mid u_t, x_{t-1}) Bel(x_{t-1}) dx_{t-1}
   $$
   - Here:
     - $p(z_t \mid x_t)$ is the likelihood of observation $z_t$ given state $x_t$.
     - $p(x_t \mid u_t, x_{t-1})$ is the transition model, representing the probability of reaching $x_t$ from $x_{t-1}$ given the action $u_t$.
     - $\eta$ is a normalization constant.
#### Characteristics
   - **Approximate Representation**: The belief is represented by particles instead of a continuous distribution.
   - **Flexibility**: It can model distributions that are non-Gaussian and apply to non-linear transformations.
   - **Survival-of-the-Fittest Principle**: Particles are re-weighted based on how well they align with observations, and particles with higher weights survive, representing likely states.
#### Mathematical Description
1. **Weighted Sample Set**:
   - The particle filter uses a set of weighted samples $\mathcal{X}_t$ to represent the probability distribution:
     $$
     \mathcal{X}_t = \{ \langle x_t^{[1]}, w_t^{[1]} \rangle, \langle x_t^{[2]}, w_t^{[2]} \rangle, \dots, \langle x_t^{[M]}, w_t^{[M]} \rangle \}
     $$
     where:
     - $x_t^{[i]}$ represents the state hypotheses (particles).
     - $w_t^{[i]}$ represents the importance weights associated with each particle.
2. **Probability Representation**:
   - The particles collectively represent the belief $p(x)$ as:
     $$
     p(x) = \sum_{i=1}^{M} w_t^{[i]} \cdot \delta_{x_t^{[i]}}(x)
     $$
   - This equation uses a Dirac delta function $\delta_{x_t^{[i]}}(x)$, which focuses probability mass at each particle’s location.
#### The Particle Filter Algorithm
![[Pasted image 20241111075339.png#invert|300]]
1. **Steps in the Algorithm**:
   - **Sample from Proposal**: For each particle, sample a new state $x_t^{[m]}$ based on the transition model $p(x_t \mid u_t, x_{t-1})$.
   - **Compute Weights**: Calculate the weight $w_t^{[m]}$ for each particle based on the observation likelihood $p(z_t \mid x_t^{[m]})$.
   - **Resampling**: Resample particles according to their weights, keeping particles with higher weights and discarding those with lower weights.
2. **Purpose**: This process refines the particles so that they concentrate in regions with high posterior probability, providing a good approximation of the robot’s belief about its state.
#### Robot Localization with Particle Filters (Monte Carlo Localization)
1. **Pose Hypotheses**: Each particle represents a possible pose (position and orientation) of the robot.
2. **Prediction Step**: The proposal distribution is based on the motion model, which predicts the robot's next pose.
3. **Correction Step**: The observation model adjusts particle weights based on sensor readings.
4. **Monte Carlo Localization**: Since this approach uses random sampling and probabilistic models, it is often referred to as Monte Carlo Localization.
#### Example
![[Pasted image 20241111075502.png#invert|600]]
1. **Uniform Distribution**: Initially, particles are distributed uniformly across all possible locations since there is no information about the robot’s position.
2. **Initial Belief**: This represents a **global localization** problem, where the robot could be anywhere within the environment.
![[Pasted image 20241111075526.png#invert|600]]
1. **Importance Weights Update**:
   - Based on sensor readings, the observation model $p(z_t \mid x_t^{[m]})$ is used to update the importance weights of particles.
   - The plot shows how the sensor information aligns particles with likely positions, concentrating them near areas with matching sensor characteristics.
![[Pasted image 20241111075544.png#invert|600]]
1. **Movement Model Application**:
   - After resampling, particles are propagated based on the motion model $p(x_t \mid u_t, x_{t-1})$.
   - This causes particles to shift according to the robot’s predicted movement, further concentrating them around probable positions.
![[Pasted image 20241111075559.png#invert|600]]

2. **Refining the Weights**:
   - Another sensor measurement refines the particle weights.
   - Only particles that match the sensor information are retained with higher weights, while others are given lower weights or removed.
![[Pasted image 20241111075621.png#invert|600]]
1. **Final Localization**:
   - After successive iterations of motion and sensor updates, the particles converge to a single, high-probability location.
   - This final concentration of particles indicates that the robot has successfully localized itself within the environment.
## Markov Chain Monte Carlo
These slides introduce **Markov Chain Monte Carlo (MCMC)**, a powerful method for sampling from complex probability distributions, especially in high-dimensional spaces where simpler methods like rejection or importance sampling become inefficient. Here’s a detailed explanation of each slide:
1. **Problem with High-Dimensional Spaces**:
   - Rejection and importance sampling become computationally inefficient in high-dimensional spaces because the probability of finding samples in high-probability regions is very low.
2. **Alternative: Markov Chain Monte Carlo (MCMC)**:
   - MCMC is an approach that constructs a **Markov chain** where the sequence of samples eventually represents the desired target distribution.
   - It differs from other methods in that it maintains a **current state**, and the next sample is chosen based on this state.
3. **Key Algorithms**:
   - **[[Metropolis-Hastings]]** and **[[Gibbs Sampling]]** are two of the most common algorithms for MCMC, each providing a different way of generating the next state in the chain.
### Markov Chains Revisited
1. **Markov Chain Definition**:
   - A Markov Chain is a sequence of random variables $X_1, X_2, \dots, X_T$ where the probability of moving to the next state depends only on the current state:
     $$
     p(X_1, \dots, X_T) = p(X_1) \prod_{t=2}^{T} p(X_t \mid X_{t-1})
     $$
   - This "memoryless" property, where each state depends only on the previous one, is key to the efficiency of MCMC.
2. **Graphical Representation**:
   - The chain is represented as a sequence of states connected by arrows, where each state $X_t$ depends only on $X_{t-1}$.
![[Pasted image 20241111080559.png#invert|300]]
3. **Transition Probabilities**:
   - The probability of moving from one state to another is represented by transition probabilities $p(X_t \mid X_{t-1})$, which can be visualized in a state transition diagram.
### The State Transition Diagram
![[Pasted image 20241111080631.png#invert|300]]
1. **State Representation**:
   - Each square represents a possible state $k$ of a time-dependent random variable at a specific time step.
2. **Transition Probabilities**:
   - For each state $k$ at time $t-2$, there is a probability $\pi_{k, t-2}$ of being in that state.
3. **Purpose of Diagram**:
   - This helps visualize how the process moves between states over time, showing that each transition depends only on the previous state.
1. **Transition Matrix**:
   - This diagram expands the transitions between states over time, introducing **transition probabilities** $A_{ij}$ that indicate the probability of moving from state $i$ at time $t-2$ to state $j$ at time $t-1$.
   - The transition probabilities can be organized in a matrix $A$, allowing us to compute the probability distribution over states at any time step using matrix multiplication:
     $$
     \pi_{t-1} = \pi_{t-2} A
     $$
### Main Idea of MCMC
1. **Using Markov Chains for Sampling**:
   - Markov chains are often used in probabilistic models like Hidden Markov Models (HMMs), but in MCMC, they are specifically designed to sample from a **target distribution**.
2. **Designing Transition Probabilities**:
   - In MCMC, we design the transition probabilities in such a way that the Markov chain’s stationary distribution (the long-term distribution it converges to) is the desired target distribution.
   - This means that by running the chain, we can generate samples that follow the target distribution.
### Important Concepts in MCMC
1. **Homogeneous Markov Chain**:
   - A Markov chain is homogeneous if the transition probabilities do not change over time, meaning they are the same at every time step $t$.
2. **Row-Stochastic Matrix**:
   - The transition matrix $A$ is row-stochastic, meaning all entries are between 0 and 1, and each row sums to 1. This ensures that the probabilities are valid and sum to 1 across all possible transitions.
3. **Vector-Matrix Multiplication**:
   - By multiplying the state distribution vector by the transition matrix, we can compute the probabilities of reaching each state in the next step.
### The Stationary Distribution
1. **Definition**:
   - The **stationary distribution** $\pi_t$ of a Markov chain is a probability distribution over states that remains unchanged as the chain progresses.
   - Mathematically, for state $k$ at time $t$, the probability $\pi_{k,t}$ is given by:
     $$
     \pi_{k,t} = \sum_{i=1}^{K} \pi_{i, t-1} A_{ik}
     $$
   - In matrix form, we can write:
     $$
     \pi_t = \pi_{t-1} A
     $$
   - **Stationary Condition**: A distribution is stationary if $\pi_t = \pi_{t-1}$.

2. **Questions**:
   - How do we know if a stationary distribution exists?
   - If it exists, is it unique?
   - How can we ensure that the Markov chain converges to this distribution?
### Existence of the Stationary Distribution
1. **Eigenvector Problem**:
   - To find the stationary distribution, we solve the eigenvector problem:
     $$
     A^T v = v
     $$
   - Here, $v$ is the eigenvector of $A^T$ with eigenvalue 1, and the stationary distribution $\pi$ is given by the normalized eigenvector $v^T$.
2. **Theorem**:
   - According to the **Perron-Frobenius theorem**, every row-stochastic matrix has an eigenvector corresponding to eigenvalue 1. However, this eigenvector is not necessarily unique unless certain conditions (e.g., irreducibility) are met.
### Uniqueness of the Stationary Distribution
1. **Conditions for Uniqueness**:
   - A Markov chain may have multiple stationary distributions. To guarantee a unique stationary distribution, the chain must be **ergodic**:
     - **Ergodicity**: Every state is reachable from any other state in a finite number of steps with non-zero probability.
   - This is equivalent to the property that the transition matrix $A$ is **irreducible**, meaning:
     $$
     \forall i, j \, \exists m \text{ such that } (A^m)_{ij} > 0
     $$
   - The diagram on this slide shows a simple example of an ergodic chain where each state is accessible from any other state.
### Convergence
1. **Convergence to the Stationary Distribution**:
   - For the chain to converge to the stationary distribution, the states must also be **aperiodic**.
   - **Aperiodic**: A chain is aperiodic if it does not return to a state in regular intervals.
2. **Theorem**:
   - If a Markov chain is **irreducible**, **aperiodic**, and has a finite number of states, it will converge to a unique stationary distribution regardless of the initial distribution.
3. **Implication**:
   - This theorem implies that we can start the chain with any initial distribution, and it will eventually converge to the stationary distribution.
### Summary of Finding a Markov Chain
1. **Summary of Conditions for Convergence**:
   - If a Markov chain has an irreducible transition matrix, there exists a unique stationary distribution.
   - An irreducible, aperiodic Markov chain will converge to this distribution over time, regardless of the initial state distribution.
2. **Reverse Problem**:
   - Often, we know the target distribution $\pi$ we want to sample from, and the task is to design a Markov chain that will converge to $\pi$.
### Detailed Balance
1. **Detailed Balance**:
   - **Definition**: A Markov chain satisfies the **detailed balance** condition if, for each pair of states $i$ and $j$,
     $$
     \pi_i A_{ij} = \pi_j A_{ji}
     $$
   - If detailed balance holds, the chain is **reversible**. This property simplifies the construction of chains that converge to a specific distribution.
2. **Interpretation**:
   - Detailed balance ensures that the flow of probability between any two states is balanced, which helps in achieving stationarity.
### Making a Distribution Stationary
1. **Theorem**:
   - If a Markov chain with transition matrix $A$ is irreducible and satisfies detailed balance with respect to a distribution $\pi$, then $\pi$ is a stationary distribution of the chain.
2. **Proof Outline**:
   - Summing over states and applying detailed balance shows that $\pi = \pi A$, proving stationarity.
3. **Sufficiency**:
   - Detailed balance is a sufficient condition for stationarity but not a necessary one. There are other ways to achieve stationarity without detailed balance.
These slides provide an explanation of **sampling with a Markov Chain** using the **Metropolis-Hastings (MH) algorithm**, which is a popular MCMC (Markov Chain Monte Carlo) method for sampling from complex distributions. Here’s a breakdown:
### Sampling with a Markov Chain
1. **Idea of MCMC**:
   - The goal of MCMC is to generate samples from a target distribution by constructing a Markov chain that has this target distribution as its stationary distribution.
   - MCMC uses a **proposal distribution** $q(x' | x)$ to suggest new states.
2. **[[Metropolis-Hastings]] Algorithm**:
   - In Metropolis-Hastings, the probability of moving to a new state $x'$ depends on both the **target distribution** and the **proposal distribution**.
   - Given the current state $x$, the probability of accepting a proposed state $x'$ is:
     $$
     \text{min}\left(1, \frac{\tilde{p}(x') q(x | x')}{\tilde{p}(x) q(x' | x)}\right)
     $$
   - Here, $\tilde{p}(x)$ is the unnormalized target distribution, and $q(x' | x)$ is the probability of proposing $x'$ given $x$.
## Hamiltonian Monte Carlo
The Hamiltonian Monte Carlo (HMC) method is a sampling technique that helps in exploring high-dimensional distributions effectively by leveraging ideas from physics, specifically Hamiltonian mechanics. Here’s a detailed breakdown of the slides:
### Motivation
- **Goal**: Sampling is used for computing expectations of a function $f$ under a probability distribution $p(x)$.
  $$
  E_p[f] = \int p(x) f(x) \, dx
  $$
- **Intuition**: Sampling values close to the mode of the target distribution $p$ gives a good approximation of the expectation.
### The Curse of Dimensionality
- **Problem**: In high-dimensional spaces:
  - The volume of a neighborhood around a point grows exponentially with dimension, making it difficult to cover with samples.
  - Most of the volume lies far from the mode, making it inefficient to sample near the mode alone.
- **Implication**: Expectations need to be computed over larger volumes, even though the density far from the mode is low.
### The Typical Set
- **Concept**: The typical set is the region of the parameter space where most of the probability mass resides.
  - **Near the mode**: High density but low volume.
  - **Far from the mode**: Low density but high volume.
  - **Intermediate region (Typical Set)**: Trade-off between density and volume, where most samples should be drawn.
- **Importance**: In high dimensions, this typical set becomes narrow, and focusing sampling within it becomes crucial for accurate expectations.
### MCMC and the Typical Set
![[Pasted image 20241111082340.png#invert|300]]
- **Metropolis-Hastings (MH) in MCMC**: MH algorithm tends to generate samples that converge towards the typical set.
- **Three Phases**:
  - **Burn-in**: Initial convergence to the typical set.
  - **Exploration**: Sampling around the typical set.
  - **Further Exploration**: Refining the exploration of the typical set.
### Observations about MCMC
- **Challenge**: MCMC with MH explores the typical set but can have slow convergence in high-dimensional spaces.
- **Problem**: The proposal distribution may suggest moves outside the typical set, resulting in frequent rejections.
- **Solution Idea**: Design algorithms to remain within the typical set explicitly, avoiding unnecessary rejections.
### Staying in the Typical Set
- **Approach**: Use a **vector field** to guide the samples and keep them within the typical set.
- **Naive Gradient Issue**: Using the gradient of the target distribution draws samples towards the mode, which can lead out of the typical set.
- **Solution**: Apply an analogy from physics to introduce momentum, preventing the samples from clustering only near the mode.
### Momentum and Parameter Space Extension
- **Hamiltonian Dynamics**:
  - Extend the parameter space to include momentum variables $(q_n, p_n)$, where $q$ represents position and $p$ represents momentum.
  - **Joint Distribution**:
    $$
    \pi(q, p) = \pi(p | q) \pi(q)
    $$
  - **Purpose**: Momentum helps maintain movement through the typical set without collapsing at the mode.
### Hamiltonian Function and Energy Decomposition
- **Hamiltonian Function** $H(q, p)$:
  $$
  \pi(q, p) = e^{-H(q, p)}
  $$
  - Represents the energy of the system, decomposed into:
    - **Kinetic Energy** $K(q, p)$ based on momentum.
    - **Potential Energy** $V(q)$ based on position.
- This energy-based approach helps maintain momentum and allows the samples to explore the typical set effectively.
### Hamiltonian Dynamics Equations
- **Equations of Motion**: Govern the updates of $q$ and $p$ over time:
  $$
  \frac{dq}{dt} = \frac{\partial H}{\partial p} = \frac{\partial K}{\partial p}
  $$
  $$
  \frac{dp}{dt} = -\frac{\partial H}{\partial q} = -\frac{\partial V}{\partial q}
  $$
- These differential equations describe how to move in the parameter space while maintaining the total energy. Following this trajectory enables efficient exploration of the typical set without being pulled too close to the mode.
### Key Steps in HMC
1. **Momentum Sampling**: First, we sample a momentum variable $p$ from a conditional distribution $\pi(p | q)$, where $q$ represents the position or state we are interested in sampling from.
2. **Hamiltonian Dynamics**: Using the Hamiltonian dynamics, we move in the phase space (position-momentum space) following a trajectory. This movement is defined by the equations of motion that preserve the energy (Hamiltonian) of the system.
3. **Projection**: After moving in phase space, we project back to the position space to obtain a new sample for $q$, which is close to the original target distribution.
### Choosing the Kinetic Energy (Proposal)
The kinetic energy term in the Hamiltonian is related to the proposal distribution. Often, a Gaussian distribution for $p$ is used, where the metric $M$ defines the "shape" or covariance of the distribution.
### Implementation with the Leapfrog Algorithm
The leapfrog algorithm is used to numerically solve the Hamiltonian equations of motion. It’s efficient and helps ensure that the HMC algorithm remains stable and accurate over many steps. The steps in the leapfrog algorithm include updating position and momentum iteratively using small steps, with adjustments based on the gradient of the log of the target density (potential energy).
### HMC in Practice
HMC is widely used for high-dimensional sampling tasks, including Bayesian neural networks, where it serves as a ground-truth method for uncertainty quantification. One popular implementation of HMC is in the Stan library, which provides tools for Bayesian inference.
