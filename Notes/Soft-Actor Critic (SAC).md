The Soft Actor-Critic (SAC) algorithm is a reinforcement learning method that combines the benefits of actor-critic methods with improvements in exploration, afforded by entropy maximization. This technique optimizes a policy that seeks to maximize not only the expected sum of rewards but also the entropy of the policy. This addition of entropy ensures more exploration by the policy, preventing premature convergence to suboptimal deterministic policies.
### Critic: “Soft” Q-Function
In SAC, the Q-function, which traditionally estimates the expected return from a state-action pair, is augmented with an entropy term to form what's known as the "soft" Q-function:

$$ 
y_t = r_t + \gamma \left( \min_{j=1,2} Q_{\phi_j}(s', a') - \alpha \log \pi(a'|s') \right)
$$
#### Key Elements:
- **Entropy Bonus:** The term $\alpha \log \pi(a' \mid s')$ where $\alpha$ is the temperature parameter that scales the importance of the entropy term against the reward. This promotes exploration by rewarding uncertainty in action selection.
- **Double Q-Learning:** SAC typically uses two Q-functions (denoted as $Q_{\phi_1}$ and $Q_{\phi_2}$) to mitigate positive bias in the policy improvement step, similar to the Clipped Double Q-Learning in TD3. It takes the minimum of these two Q-functions to reduce overestimation.
### Actor: Policy Optimization
The policy in SAC is updated to maximize a trade-off between expected return and entropy, encouraging the policy to explore more widely:

$$ 
J_{SAC}(\theta) = \mathbb{E}_{s \sim \mathcal{D}} \left[ \mathbb{E}_{a \sim \pi_\theta} \left[ Q_\phi(s, a) - \alpha \log \pi_\theta(a \mid s) \right] \right]
$$
#### Optimization Techniques:
- **Reparameterization Trick:** SAC uses this trick to improve sample efficiency and reduce variance in policy gradients. Actions are sampled according to $a = f_\theta(\epsilon; s)$ where $\epsilon$ is an input noise vector, and $f_\theta$ is a deterministic function. This allows gradients to pass through the stochastic node of action sampling.
### Reparameterization Trick
The reparameterization trick enables the gradients of the stochastic policy to be computed more effectively:

$$ 
a = \mu_\theta(s) + \sigma_\theta(s) \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
$$

where $\mu$ and $\sigma$ are the outputs of the policy network, and $\epsilon$ is an input noise vector. This formulation helps in computing gradients directly with respect to policy parameters $\theta$ by making the randomness independent of the parameters.
### Actor Update in SAC
#### Actor Objective
The objective function for the actor in SAC aims to maximize the expected reward while also maximizing entropy to encourage exploration. The mathematical formulation provided in your image is:

$$ J_{SAC}(\theta) \approx \frac{1}{N} \sum_{i} \mathbb{E}_{a \sim \pi_\theta} \left[Q_\phi(s_i, a) - \alpha \log \pi_\theta(a \mid s_i)\right] $$

Here:
- $\pi_\theta(a \mid s)$ is the policy parameterized by $\theta$.
- $Q_\phi(s, a)$ is the soft Q-function that outputs the expected return of taking action $a$ in state $s$, adjusted by the entropy term.
- $\alpha$ is the temperature parameter that controls the trade-off between entropy and reward.
#### Reparameterization Trick
- **Purpose**: This trick is used to allow backpropagation through stochastic nodes. It helps to compute gradients of the policy parameters $\theta$ efficiently.
- **Implementation**: Assume the policy $\pi_\theta$ is Gaussian, actions are sampled as $a = \mu_\theta(s) + \sigma_\theta(s) \cdot \epsilon$, where $\epsilon \sim \mathcal{N}(0, I)$. Here, $\mu_\theta(s)$ and $\sigma_\theta(s)$ are outputs of the neural network representing the mean and standard deviation of the policy's action distribution.
- **Gradient Calculation**: The reparameterization allows the gradient of the objective with respect to the parameters $\theta$ to be computed as if $a$ were deterministically generated by $\theta$ and $\epsilon$.
### Advantages of SAC
1. **Exploration-Exploitation Balance:** By maximizing entropy, SAC naturally balances exploration and exploitation, leading to more robust learning in environments with sparse or deceptive rewards.
2. **Stability and Convergence:** The use of double Q-functions and the reparameterization trick contributes to the stability of the learning process and often leads to faster convergence compared to methods that do not incorporate these features.
3. **Continuous Action Spaces:** SAC is particularly well-suited for tasks with continuous action spaces, making it ideal for complex control tasks such as robotics.
## SAC Algorithm
![[Pasted image 20240708123401.png#invert|600]]
1. **Data Collection**: Interact with the environment using the current policy and store the tuples $(s_i, a_i, r_i, s'_i)$ in a replay buffer.
2. **Sample a Batch**: Randomly sample a batch of transitions from the buffer to use for training.
3. **Compute Target Values**: For each sampled transition, compute the target value for the Q-function updates using:
   $$ y_i = r_i + \gamma \left( \min_{j=1,2} Q_{\phi_j}(s'_i, a') - \alpha \log \pi(a' \mid s'_i) \right) $$
   where $a'$ is sampled from the current policy.
4. **Update Q-Function**: Adjust the parameters of the Q-functions by minimizing the mean squared error between the current Q-function estimates and the computed target values.
5. **Update Policy**: Use the reparameterization trick and the policy gradient derived from the actor's objective to update the policy parameters $\theta$.
6. **Update Target Networks**: Use Polyak averaging to slowly update the target Q-function parameters towards the current Q-function parameters. This stabilizes learning by making the target function change more smoothly.
